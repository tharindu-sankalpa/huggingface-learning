{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9329a5-64f5-46c0-8a19-5bc7e548e4dc",
   "metadata": {},
   "source": [
    "# üöÄ Qwen Model Benchmarking on GPU (H100, T4, L4)\n",
    "A hands-on exploration of Qwen LLM variants using Hugging Face Transformers and PyTorch with GPU performance metrics.\n",
    "\n",
    "\n",
    "## üß™ Environment Check: Python Setup\n",
    "Before starting any model benchmarking, it's important to verify the Python environment. This cell prints the path of the current Python executable and the Python version being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d8fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /anaconda/envs/qwen-llm/bin/python\n",
      "Python version: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff984a9",
   "metadata": {},
   "source": [
    "# System Resource Assessment\n",
    "Start by examining the available resources and verifying our environment setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ddc8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027286d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA H100 NVL', major=9, minor=0, total_memory=95248MB, multi_processor_count=132, uuid=53317aba-93d7-f441-1b59-0b07d076e10e, L2_cache_size=60MB)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6b8c77-27fa-41fb-a346-3669d0332f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA H100 NVL'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96ddfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0645bb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.6921806335449"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory().total/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ec70f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.18262100219727"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory().available/(1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1b580-9530-4f67-b011-a6391563d0b7",
   "metadata": {},
   "source": [
    "# NVIDIA H100 NVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a0c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python executable: /anaconda/envs/qwen-llm/bin/python\n",
      "üî• PyTorch version: 2.7.1+cu118\n",
      "ü§ñ Transformers version: 4.44.0\n",
      "üìÖ Benchmark timestamp: 2025-08-02T15:06:42.704293\n",
      "============================================================\n",
      "üéÆ GPU 0: NVIDIA H100 NVL\n",
      "   üíæ Memory: 93.02 GB\n",
      "   üîß Compute capability: 9.0\n",
      "üñ•Ô∏è  CPU cores: 40\n",
      "üß† Total RAM: 314.69 GB\n",
      "üíö Available RAM: 235.27 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Verify environment setup\n",
    "print(f\"üêç Python executable: {sys.executable}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers version: {__import__('transformers').__version__}\")\n",
    "print(f\"üìÖ Benchmark timestamp: {datetime.now().isoformat()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Comprehensive system resource assessment\"\"\"\n",
    "    info = {\n",
    "        \"cpu_cores\": psutil.cpu_count(),\n",
    "        \"total_ram_gb\": psutil.virtual_memory().total / (1024**3),\n",
    "        \"available_ram_gb\": psutil.virtual_memory().available / (1024**3),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        info[\"cuda_version\"] = torch.version.cuda\n",
    "        info[\"gpu_count\"] = torch.cuda.device_count()\n",
    "        info[\"gpus\"] = []\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            gpu_info = {\n",
    "                \"id\": i,\n",
    "                \"name\": gpu_props.name,\n",
    "                \"memory_gb\": gpu_props.total_memory / (1024**3),\n",
    "                \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\",\n",
    "                \"multiprocessor_count\": gpu_props.multi_processor_count\n",
    "            }\n",
    "            info[\"gpus\"].append(gpu_info)\n",
    "            print(f\"üéÆ GPU {i}: {gpu_props.name}\")\n",
    "            print(f\"   üíæ Memory: {gpu_props.total_memory / (1024**3):.2f} GB\")\n",
    "            print(f\"   üîß Compute capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available - check GPU drivers and PyTorch installation\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  CPU cores: {info['cpu_cores']}\")\n",
    "    print(f\"üß† Total RAM: {info['total_ram_gb']:.2f} GB\")\n",
    "    print(f\"üíö Available RAM: {info['available_ram_gb']:.2f} GB\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Get and store system information\n",
    "system_info = get_system_info()\n",
    "if system_info is None:\n",
    "    print(\"\\nüö® GPU setup required before continuing. Check environment configuration.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90a68f-ec7e-4ed8-8c03-3cfe92bf972c",
   "metadata": {},
   "source": [
    "# Tesla T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df30f744-4b28-49cf-b203-b50095146255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python executable: /anaconda/envs/qwen-llm/bin/python\n",
      "üî• PyTorch version: 2.7.1+cu118\n",
      "ü§ñ Transformers version: 4.44.0\n",
      "üìÖ Benchmark timestamp: 2025-08-02T17:32:55.451092\n",
      "============================================================\n",
      "üéÆ GPU 0: Tesla T4\n",
      "   üíæ Memory: 15.57 GB\n",
      "   üîß Compute capability: 7.5\n",
      "üñ•Ô∏è  CPU cores: 8\n",
      "üß† Total RAM: 54.92 GB\n",
      "üíö Available RAM: 53.07 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Verify environment setup\n",
    "print(f\"üêç Python executable: {sys.executable}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers version: {__import__('transformers').__version__}\")\n",
    "print(f\"üìÖ Benchmark timestamp: {datetime.now().isoformat()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Comprehensive system resource assessment\"\"\"\n",
    "    info = {\n",
    "        \"cpu_cores\": psutil.cpu_count(),\n",
    "        \"total_ram_gb\": psutil.virtual_memory().total / (1024**3),\n",
    "        \"available_ram_gb\": psutil.virtual_memory().available / (1024**3),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        info[\"cuda_version\"] = torch.version.cuda\n",
    "        info[\"gpu_count\"] = torch.cuda.device_count()\n",
    "        info[\"gpus\"] = []\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            gpu_info = {\n",
    "                \"id\": i,\n",
    "                \"name\": gpu_props.name,\n",
    "                \"memory_gb\": gpu_props.total_memory / (1024**3),\n",
    "                \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\",\n",
    "                \"multiprocessor_count\": gpu_props.multi_processor_count\n",
    "            }\n",
    "            info[\"gpus\"].append(gpu_info)\n",
    "            print(f\"üéÆ GPU {i}: {gpu_props.name}\")\n",
    "            print(f\"   üíæ Memory: {gpu_props.total_memory / (1024**3):.2f} GB\")\n",
    "            print(f\"   üîß Compute capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available - check GPU drivers and PyTorch installation\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  CPU cores: {info['cpu_cores']}\")\n",
    "    print(f\"üß† Total RAM: {info['total_ram_gb']:.2f} GB\")\n",
    "    print(f\"üíö Available RAM: {info['available_ram_gb']:.2f} GB\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Get and store system information\n",
    "system_info = get_system_info()\n",
    "if system_info is None:\n",
    "    print(\"\\nüö® GPU setup required before continuing. Check environment configuration.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466f1c3f-3a9e-4e61-9129-454d013d7466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  2 14:55:05 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 NVL                On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              62W / 400W |      3MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651a8b2d-143f-45d5-a2b9-6e9b566e0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  2 17:32:34 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   36C    P8              10W /  70W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9f764",
   "metadata": {},
   "source": [
    "# Troubleshooting Common Issues\n",
    "If you encounter CUDA-related errors, verify these common issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e99782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Troubleshooting GPU Setup ===\n",
      "‚úÖ PyTorch imported successfully\n",
      "   CUDA compiled version: 11.8\n",
      "   CUDA runtime version: (9, 0)\n",
      "‚úÖ GPU accessible: NVIDIA H100 NVL\n",
      "‚úÖ GPU tensor operations working\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common troubleshooting checks\n",
    "def troubleshoot_setup():\n",
    "    print(\"=== Troubleshooting GPU Setup ===\")\n",
    "    \n",
    "    # Check CUDA installation\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"‚úÖ PyTorch imported successfully\")\n",
    "        print(f\"   CUDA compiled version: {torch.version.cuda}\")\n",
    "        print(f\"   CUDA runtime version: {torch.cuda.get_device_capability()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PyTorch import error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check GPU accessibility\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        print(f\"‚úÖ GPU accessible: {torch.cuda.get_device_name(device)}\")\n",
    "        \n",
    "        # Test basic tensor operations\n",
    "        try:\n",
    "            test_tensor = torch.randn(1000, 1000).cuda()\n",
    "            result = torch.mm(test_tensor, test_tensor.t())\n",
    "            print(f\"‚úÖ GPU tensor operations working\")\n",
    "            del test_tensor, result  # Free memory\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå GPU tensor operations failed: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run troubleshooting if needed\n",
    "troubleshoot_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc801ab",
   "metadata": {},
   "source": [
    "# Loading and Testing Qwen2.5-7B-Instruct\n",
    "Now that our environment is verified, let's load the model and test its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ad2f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-Coder-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:08<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 66.60 GB\n",
      "üî• GPU Memory - Allocated: 66.60 GB, Reserved: 66.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model with T4-optimized settings\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "custom_cache_dir = \"/dev/shm\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with T4 GPU optimization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìç Device: {model.device}\")\n",
    "print(f\"üíæ Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "# Check GPU memory usage after loading\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"üî• GPU Memory - Allocated: {gpu_memory:.2f} GB, Reserved: {gpu_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5849bdd",
   "metadata": {},
   "source": [
    "# Basic Text Generation Testing\n",
    "Let's test the model with simple text generation to ensure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227a2b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 45.12s\n",
      "üíæ Memory delta: 0.000 GB\n",
      "üìù Generated tokens: 1119\n",
      "‚ö° Tokens/second: 24.8\n",
      "\n",
      "üìÑ Generated text:\n",
      "Kubernetes is a powerful open-source platform that automates deployment, scaling, and management of containerized applications. While it is primarily used for web applications, Kubernetes can also be effectively utilized for machine learning (ML) workloads. Here are some benefits of using Kubernetes for ML workloads:\n",
      "\n",
      "1. Scalability: Kubernetes provides the ability to scale up or down the number of nodes in a cluster based on demand, which is essential for handling the varying computational requirements of ML workloads. For example, training a large deep learning model may require a lot of resources, while serving predictions may require fewer resources.\n",
      "\n",
      "2. Resource Management: Kubernetes allows efficient allocation and utilization of resources across different ML workloads, ensuring that each workload gets the resources it needs without over-provisioning or under-provisioning. This helps in optimizing costs and improving performance.\n",
      "\n",
      "3. Flexibility: Kubernetes supports various containerization platforms, such as Docker, and can run on different infrastructure providers, including cloud and on-premises environments. This flexibility enables organizations to choose the best tools and infrastructure for their ML workloads.\n",
      "\n",
      "4. Portability: Kubernetes provides portability across different environments, making it easy to move ML workloads between development, testing, and production stages without any changes to the code or configuration.\n",
      "\n",
      "5. Fault Tolerance: Kubernetes has built-in fault tolerance mechanisms that automatically restart failed containers, replace unhealthy nodes, and ensure high availability of ML workloads. This helps in reducing downtime and improving reliability.\n",
      "\n",
      "6. Security: Kubernetes offers various security features, such as network policies, RBAC, and secrets management, that help in securing ML workloads and protecting sensitive data.\n",
      "\n",
      "7. Monitoring and Logging: Kubernetes provides built-in monitoring and logging capabilities that help in tracking the performance and health of ML workloads, identifying issues, and troubleshooting problems.\n",
      "\n",
      "In summary, Kubernetes provides several benefits for ML workloads, including scalability, resource management, flexibility, portability, fault tolerance, security, and monitoring and logging. These benefits make Kubernetes an attractive choice for organizations looking to deploy and manage ML workloads at scale.\n",
      "Absolutely, Kubernetes offers a robust framework for managing machine learning workloads, providing a range of advantages that can significantly enhance the efficiency and effectiveness of ML operations. Here's a more detailed breakdown of these benefits:\n",
      "\n",
      "### 1. **Scalability**\n",
      "- **Dynamic Scaling**: Kubernetes can automatically scale the number of pods (containers) up or down based on the current load and resource usage. This is particularly useful for ML tasks like training models, where resource demands can vary widely.\n",
      "- **Cluster Autoscaler**: This feature adjusts the size of the cluster by adding or removing worker nodes, ensuring that you have the right amount of resources available to handle your workload efficiently.\n",
      "\n",
      "### 2. **Resource Management**\n",
      "- **Efficient Resource Allocation**: Kubernetes uses resource requests and limits to allocate CPU, memory, and other resources to containers, ensuring that each workload gets the necessary resources without wasting them.\n",
      "- **Quality of Service (QoS)**: Kubernetes provides QoS classes (BestEffort, Burstable, Guaranteed) to prioritize critical workloads and manage resource contention effectively.\n",
      "\n",
      "### 3. **Flexibility**\n",
      "- **Containerization Support**: Kubernetes supports multiple containerization platforms, allowing you to use the best tool for your specific needs.\n",
      "- **Infrastructure Agnostic**: You can deploy Kubernetes clusters on various infrastructure providers, including AWS, Azure, Google Cloud, and on-premises servers, providing flexibility in how you manage your ML workloads.\n",
      "\n",
      "### 4. **Portability**\n",
      "- **Environment Consistency**: By using containers, Kubernetes ensures that your ML workloads run consistently across different environments, from development to production.\n",
      "- **Easy Deployment**: Kubernetes simplifies the process of deploying and updating applications, making it easier to move workloads between stages of the ML lifecycle.\n",
      "\n",
      "### 5. **Fault Tolerance**\n",
      "- **Self-Healing**: Kubernetes automatically restarts failed containers and replaces unhealthy nodes, ensuring that your ML services remain available and reliable.\n",
      "- **High Availability**: Features like rolling updates and rolling back allow you to update your applications without downtime, maintaining service continuity.\n",
      "\n",
      "### 6. **Security**\n",
      "- **Network Policies**: Kubernetes provides fine-grained control over network traffic, allowing you to define rules for how containers can communicate with each other.\n",
      "- **Role-Based Access Control (RBAC)**: RBAC helps in managing permissions and access to resources within the cluster, enhancing security.\n",
      "- **Secrets Management**: Kubernetes securely stores and manages sensitive information like API keys and passwords, protecting your ML workloads from unauthorized access.\n",
      "\n",
      "### 7. **Monitoring and Logging**\n",
      "- **Built-in Tools**: Kubernetes integrates with popular monitoring and logging tools like Prometheus, Grafana, and ELK Stack, providing comprehensive visibility into the performance and health of your ML workloads.\n",
      "- **Automatic Metrics Collection**: Kubernetes automatically collects metrics about resource usage and application performance, helping you identify and address issues proactively.\n",
      "\n",
      "### Additional Benefits\n",
      "- **Cost Optimization**: By efficiently managing resources and supporting spot instances, Kubernetes can help reduce cloud costs.\n",
      "- **Community and Ecosystem**: Kubernetes has a large and active community, along with a rich ecosystem of tools and extensions, providing extensive support and resources for ML workloads.\n",
      "\n",
      "In conclusion, Kubernetes offers a comprehensive solution for managing machine learning workloads, addressing key challenges related to scalability, resource management, flexibility, security, and monitoring. Its ability to adapt to varying workloads and its strong support for containerization make it an ideal choice for modern ML operations.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=200, temperature=0.7, top_p=0.8):\n",
    "    \"\"\"Basic text generation function\"\"\"\n",
    "    \n",
    "    # Encode input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    # Track GPU memory before generation\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    new_text = response[len(prompt):].strip()\n",
    "\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {len(outputs[0]) - len(inputs[0])}\")\n",
    "    print(f\"‚ö° Tokens/second: {(len(outputs[0]) - len(inputs[0])) / generation_time:.1f}\")\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "# Test basic generation\n",
    "test_prompt = \"Explain the benefits of using Kubernetes for machine learning workloads:\"\n",
    "result = generate_text(test_prompt, max_length=2000)\n",
    "print(f\"\\nüìÑ Generated text:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6270e08-332e-41a5-bfe1-ba17db095a9b",
   "metadata": {},
   "source": [
    "# üöÄ Model Benchmarking Framework\n",
    "\n",
    "## Overview\n",
    "\n",
    "This framework provides a **comprehensive and flexible setup** for benchmarking large language models (LLMs) across different:\n",
    "\n",
    "* ‚úÖ **Model variants** (e.g., base vs. instruct, 7B vs. 32B)\n",
    "* ‚úÖ **Hardware setups** (e.g., NVIDIA T4, A100, etc.)\n",
    "* ‚úÖ **Performance metrics** (e.g., GPU memory usage, latency, throughput)\n",
    "\n",
    "Its primary goal is to help you **evaluate and compare model behavior and efficiency** in different inference environments.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Features\n",
    "\n",
    "* üîÅ **Plug-and-play model loading** with Hugging Face `transformers`\n",
    "* üß† **Automatic device mapping** for multi-GPU compatibility\n",
    "* üïí **Precise generation timing**\n",
    "* üìä **Memory usage tracking** (allocated vs. reserved)\n",
    "* üìà **Tokens/sec throughput reporting**\n",
    "* üìÇ **Cache control** via custom directory (e.g., `/dev/shm` for RAM disk)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Usage Example\n",
    "\n",
    "```python\n",
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Components\n",
    "\n",
    "| Component              | Description                                                                    |\n",
    "| ---------------------- | ------------------------------------------------------------------------------ |\n",
    "| `load_and_generate()`  | Unified function to load model, generate output, and benchmark runtime metrics |\n",
    "| `AutoTokenizer`        | Dynamically loads tokenizer with optional padding fallback                     |\n",
    "| `AutoModelForCausalLM` | Loads the model using half-precision (FP16) and maps to available GPUs         |\n",
    "| GPU Tracker            | Measures memory delta before/after inference                                   |\n",
    "| Metrics Output         | Includes generation time, memory usage, token throughput                       |\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Notes\n",
    "\n",
    "* For best performance on **NVIDIA T4**, use FP16 models and limit max sequence lengths.\n",
    "* Use `/dev/shm` as `cache_dir` to improve disk I/O speed on Linux systems.\n",
    "* Consider adding `torch.cuda.reset_peak_memory_stats()` and `torch.cuda.max_memory_allocated()` if peak memory stats are needed.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this exported as a `.md` file or documented in a Jupyter notebook with visual charts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3f0202-48f4-4eac-ac12-2bb08c85f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_and_generate(model_name, prompt, max_length=200, temperature=0.7, top_p=0.8, cache_dir=\"/dev/shm\"):\n",
    "    \"\"\"\n",
    "    Load a model and tokenizer, then generate text from a prompt.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): Hugging Face model identifier\n",
    "        prompt (str): Input prompt for text generation\n",
    "        max_length (int): Maximum number of tokens in output\n",
    "        temperature (float): Sampling temperature\n",
    "        top_p (float): Nucleus sampling threshold\n",
    "        cache_dir (str): Directory to cache model and tokenizer\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\nüöÄ Loading model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìç Device: {model.device}\")\n",
    "    print(f\"üíæ Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"üî• GPU Memory - Allocated: {gpu_memory:.2f} GB, Reserved: {gpu_reserved:.2f} GB\")\n",
    "\n",
    "    # Encode input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    new_text = response[len(prompt):].strip()\n",
    "\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {len(outputs[0]) - len(inputs['input_ids'][0])}\")\n",
    "    print(f\"‚ö° Tokens/second: {(len(outputs[0]) - len(inputs['input_ids'][0])) / generation_time:.1f}\")\n",
    "\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c2b93-a272-4092-9fe7-2d41ddd7bb99",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder-7B-Instruct (NVIDIA H100 NVL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf433579-4fe3-41a1-912a-0e969f4e1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Loading model: Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 15.70 GB\n",
      "üî• GPU Memory - Allocated: 82.41 GB, Reserved: 82.65 GB\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "üïí Generation time: 5.39s\n",
      "üíæ Memory delta: 0.000 GB\n",
      "üìù Generated tokens: 311\n",
      "‚ö° Tokens/second: 57.7\n"
     ]
    }
   ],
   "source": [
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782db3a3-c9ee-4e72-b539-11cd5e388591",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder-7B-Instruct (NVIDIA Tesla T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2c631-f114-4193-90f5-22fd0599f1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Loading model: Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:54<00:00, 13.69s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.02it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 15.70 GB\n",
      "üî• GPU Memory - Allocated: 13.72 GB, Reserved: 13.96 GB\n"
     ]
    }
   ],
   "source": [
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d590d-7ec1-4b24-b83f-21a1f6ec3f1f",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder-32B-Instruct (NVIDIA H100 NVL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a176017-86b4-4677-8384-f3df5e81ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Loading model: Qwen/Qwen2.5-Coder-32B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:08<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 66.60 GB\n",
      "üî• GPU Memory - Allocated: 66.60 GB, Reserved: 66.87 GB\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 16.62s\n",
      "üíæ Memory delta: 0.034 GB\n",
      "üìù Generated tokens: 402\n",
      "‚ö° Tokens/second: 24.2\n"
     ]
    }
   ],
   "source": [
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7ea28",
   "metadata": {},
   "source": [
    "# Chat Completion Format Testing\n",
    "Since we're building an API that mimics OpenAI's chat completion format, let's test the model with proper chat formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc80439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing chat completion format...\n",
      "\n",
      "üìã Performance Metrics:\n",
      "   ‚è±Ô∏è  Generation time: 28.54s\n",
      "   üì• Input tokens: 46\n",
      "   üì§ Output tokens: 680\n",
      "   ‚ö° Speed: 23.8 tokens/sec\n",
      "   üíæ Memory used: 33.6 MB\n",
      "\n",
      "ü§ñ Assistant Response:\n",
      "Deploying Large Language Models (LLMs) on Kubernetes offers several key advantages over traditional VM-based deployments, particularly in the context of scalability, efficiency, and operational flexibility. Here are some of the primary benefits:\n",
      "\n",
      "1. **Scalability**:\n",
      "   - **Horizontal Scaling**: Kubernetes allows for easy horizontal scaling of LLM workloads. You can scale up or down the number of pods (instances) running your model based on demand, which is crucial for handling variable loads typical in applications using LLMs.\n",
      "   - **Auto-scaling**: With Kubernetes, you can set up auto-scaling policies that automatically adjust the number of replicas of your application based on CPU/memory usage or custom metrics, ensuring optimal resource utilization.\n",
      "\n",
      "2. **Resource Efficiency**:\n",
      "   - **Efficient Resource Allocation**: Kubernetes uses containers, which are lightweight and share the host OS kernel, allowing for more efficient use of resources compared to full VMs. This means you can run more instances of your LLM on the same hardware.\n",
      "   - **Fine-grained Resource Management**: You can specify precise CPU and memory requirements for each container, optimizing resource allocation and reducing costs.\n",
      "\n",
      "3. **Operational Flexibility**:\n",
      "   - **Declarative Configuration**: Kubernetes uses declarative configuration files (YAML) to define the desired state of your application, making it easier to manage and replicate environments across different stages (development, testing, production).\n",
      "   - **Continuous Integration/Continuous Deployment (CI/CD)**: Kubernetes integrates well with CI/CD pipelines, allowing for rapid deployment and updates of your LLM applications with minimal downtime.\n",
      "\n",
      "4. **High Availability and Fault Tolerance**:\n",
      "   - **Self-healing**: Kubernetes automatically restarts failed containers, replaces and reschedules them on healthy nodes, and kills containers that don‚Äôt respond to user-defined health checks.\n",
      "   - **Load Balancing**: Kubernetes provides built-in load balancing to distribute traffic across multiple instances of your application, improving availability and reliability.\n",
      "\n",
      "5. **Security**:\n",
      "   - **Isolation**: Containers provide better isolation between different applications and services, reducing the risk of security breaches.\n",
      "   - **Network Policies**: Kubernetes supports network policies to control traffic flow at the IP address or port level, enhancing security.\n",
      "\n",
      "6. **Monitoring and Logging**:\n",
      "   - **Built-in Monitoring**: Tools like Prometheus and Grafana can be integrated with Kubernetes to monitor application performance and resource usage in real-time.\n",
      "   - **Centralized Logging**: Kubernetes supports centralized logging solutions like ELK Stack or Fluentd, making it easier to aggregate and analyze logs from all containers.\n",
      "\n",
      "7. **Cost-Effectiveness**:\n",
      "   - **Pay-as-you-go Model**: By efficiently utilizing resources and scaling only when necessary, Kubernetes can help reduce cloud infrastructure costs.\n",
      "   - **Spot Instances**: Kubernetes can be configured to use spot instances, which are significantly cheaper than on-demand instances, providing a cost-effective way to run LLMs.\n",
      "\n",
      "8. **Ease of Use and Community Support**:\n",
      "   - **Extensive Documentation and Community**: Kubernetes has extensive documentation and a large community, making it easier to find support and best practices for deploying and managing LLMs.\n",
      "\n",
      "In summary, Kubernetes provides a robust, scalable, and flexible platform for deploying LLMs, offering significant advantages over traditional VM-based deployments in terms of resource management, operational efficiency, and cost-effectiveness.\n"
     ]
    }
   ],
   "source": [
    "def format_chat_prompt(messages):\n",
    "    \"\"\"Format messages for Qwen2.5-Instruct chat template\"\"\"\n",
    "    \n",
    "    # Qwen2.5-Instruct uses a specific chat format\n",
    "    formatted_prompt = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        \n",
    "        if role == \"system\":\n",
    "            formatted_prompt += f\"<|im_start|>system\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"user\":\n",
    "            formatted_prompt += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            formatted_prompt += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n",
    "    \n",
    "    # Add assistant start token for generation\n",
    "    formatted_prompt += \"<|im_start|>assistant\\n\"\n",
    "    return formatted_prompt\n",
    "\n",
    "def chat_completion(messages, max_tokens=500, temperature=0.7, top_p=0.8):\n",
    "    \"\"\"OpenAI-style chat completion function\"\"\"\n",
    "    \n",
    "    # Format the conversation\n",
    "    prompt = format_chat_prompt(messages)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = len(inputs[0])\n",
    "    \n",
    "    # Track performance metrics\n",
    "    start_time = time.time()\n",
    "    memory_before = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_tokens,  # Use max_new_tokens instead of max_length\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            stop_strings=[\"<|im_end|>\"],  # Stop at chat format end token\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated()\n",
    "    total_tokens = len(outputs[0])\n",
    "    new_tokens = total_tokens - input_length\n",
    "    \n",
    "    # Decode and clean response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_response = full_response[len(tokenizer.decode(inputs[0], skip_special_tokens=True)):].strip()\n",
    "    \n",
    "    # Remove chat format tokens if present\n",
    "    if assistant_response.endswith(\"<|im_end|>\"):\n",
    "        assistant_response = assistant_response[:-11].strip()\n",
    "    \n",
    "    # Performance metrics\n",
    "    tokens_per_second = new_tokens / generation_time if generation_time > 0 else 0\n",
    "    memory_used_mb = (memory_after - memory_before) / 1e6\n",
    "    \n",
    "    return {\n",
    "        \"response\": assistant_response,\n",
    "        \"metrics\": {\n",
    "            \"generation_time\": generation_time,\n",
    "            \"input_tokens\": input_length,\n",
    "            \"output_tokens\": new_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_second\": tokens_per_second,\n",
    "            \"memory_used_mb\": memory_used_mb\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test chat completion format\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a helpful AI assistant specializing in cloud infrastructure and DevOps.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What are the key advantages of deploying LLMs on Kubernetes compared to traditional VM-based deployments?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing chat completion format...\")\n",
    "result = chat_completion(test_messages, max_tokens=1000)\n",
    "\n",
    "print(f\"\\nüìã Performance Metrics:\")\n",
    "print(f\"   ‚è±Ô∏è  Generation time: {result['metrics']['generation_time']:.2f}s\")\n",
    "print(f\"   üì• Input tokens: {result['metrics']['input_tokens']}\")\n",
    "print(f\"   üì§ Output tokens: {result['metrics']['output_tokens']}\")\n",
    "print(f\"   ‚ö° Speed: {result['metrics']['tokens_per_second']:.1f} tokens/sec\")\n",
    "print(f\"   üíæ Memory used: {result['metrics']['memory_used_mb']:.1f} MB\")\n",
    "\n",
    "print(f\"\\nü§ñ Assistant Response:\\n{result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7cb3da",
   "metadata": {},
   "source": [
    "# Code Generation Testing Framework\n",
    "Let's create a comprehensive testing framework for coding capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Code Generation Test:\n",
      "==================================================\n",
      "Instruction: Create a Python function that implements binary search on a sorted list\n",
      "\n",
      "üìä Performance:\n",
      "  Time: 25.90s\n",
      "  Speed: 23.9 tok/s\n",
      "  Tokens: 618\n",
      "\n",
      "üíª Generated Code:\n",
      "Certainly! Below is a Python function that implements the binary search algorithm on a sorted list. The function returns the index of the target element if it is found in the list, and `-1` if the target is not present.\n",
      "\n",
      "```python\n",
      "def binary_search(sorted_list, target):\n",
      "    \"\"\"\n",
      "    Perform binary search on a sorted list to find the index of the target element.\n",
      "\n",
      "    Parameters:\n",
      "    sorted_list (list): A list of elements sorted in ascending order.\n",
      "    target: The element to search for in the list.\n",
      "\n",
      "    Returns:\n",
      "    int: The index of the target element if found, otherwise -1.\n",
      "    \"\"\"\n",
      "    left, right = 0, len(sorted_list) - 1\n",
      "\n",
      "    while left <= right:\n",
      "        mid = left + (right - left) // 2  # Calculate the middle index\n",
      "\n",
      "        # Check if the target is present at mid\n",
      "        if sorted_list[mid] == target:\n",
      "            return mid\n",
      "        # If target is greater, ignore the left half\n",
      "        elif sorted_list[mid] < target:\n",
      "            left = mid + 1\n",
      "        # If target is smaller, ignore the right half\n",
      "        else:\n",
      "            right = mid - 1\n",
      "\n",
      "    # Target is not present in the list\n",
      "    return -1\n",
      "\n",
      "# Example usage:\n",
      "if __name__ == \"__main__\":\n",
      "    sorted_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "    target = 7\n",
      "    result = binary_search(sorted_list, target)\n",
      "    \n",
      "    if result != -1:\n",
      "        print(f\"Element {target} is present at index {result}.\")\n",
      "    else:\n",
      "        print(f\"Element {target} is not present in the list.\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **Parameters**:\n",
      "  - `sorted_list`: A list of elements that must be sorted in ascending order.\n",
      "  - `target`: The element you want to find in the list.\n",
      "\n",
      "- **Returns**:\n",
      "  - The index of the `target` element if it is found in the list.\n",
      "  - `-1` if the `target` is not found in the list.\n",
      "\n",
      "- **Algorithm**:\n",
      "  - Initialize two pointers, `left` and `right`, to the start and end of the list, respectively.\n",
      "  - While `left` is less than or equal to `right`:\n",
      "    - Calculate the middle index `mid`.\n",
      "    - Compare the middle element with the target:\n",
      "      - If they are equal, return `mid`.\n",
      "      - If the middle element is less than the target, move the `left` pointer to `mid + 1`.\n",
      "      - If the middle element is greater than the target, move the `right` pointer to `mid - 1`.\n",
      "  - If the loop ends without finding the target, return `-1`.\n",
      "\n",
      "This implementation is efficient with a time complexity of \\(O(\\log n)\\), where \\(n\\) is the number of elements in the list.\n"
     ]
    }
   ],
   "source": [
    "def format_coding_prompt(instruction, code_context=\"\", language=\"python\"):\n",
    "    \"\"\"Format coding prompts for Qwen2.5-Coder\"\"\"\n",
    "    \n",
    "    system_message = (\n",
    "        \"You are a helpful coding assistant. You provide accurate, efficient, and well-documented code solutions.\\n\"\n",
    "        \"When writing code, follow these principles:\\n\"\n",
    "        \"1. Write clean, readable code with proper formatting\\n\"\n",
    "        \"2. Include helpful comments where necessary\\n\"\n",
    "        \"3. Follow language-specific best practices\\n\"\n",
    "        \"4. Provide complete, working solutions\"\n",
    "    )\n",
    "\n",
    "    if code_context:\n",
    "        user_message = f\"\"\"Language: {language}\n",
    "\n",
    "        Context:\n",
    "        ```{language}\n",
    "        {code_context}\n",
    "        ``` {{data-source-line=\"318\"}}\n",
    "\n",
    "        Request: {instruction}\"\"\"\n",
    "    else:\n",
    "        user_message = f\"\"\"Language: {language}\n",
    "\n",
    "        Request: {instruction}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    formatted_prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        formatted_prompt += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "\n",
    "    formatted_prompt += \"<|im_start|>assistant\\n\"\n",
    "    return formatted_prompt\n",
    "\n",
    "def generate_code(instruction, code_context=\"\", language=\"python\", max_tokens=2000, temperature=0.2):\n",
    "    \"\"\"Generate code with performance metrics\"\"\"\n",
    "    \n",
    "    prompt = format_coding_prompt(instruction, code_context, language)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    start_time = time.time()\n",
    "    memory_before = torch.cuda.memory_allocated()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            stop_strings=[\"<|im_end|>\", \"<|endoftext|>\"],\n",
    "            tokenizer=tokenizer  # ‚úÖ required when using stop_strings\n",
    "        )\n",
    "\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated()\n",
    "    total_tokens = outputs.shape[1]\n",
    "    new_tokens = total_tokens - input_length\n",
    "\n",
    "    decoded_prompt = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    code_response = decoded_output[len(decoded_prompt):].strip()\n",
    "\n",
    "    for stop in [\"<|im_end|>\", \"<|endoftext|>\"]:\n",
    "        if stop in code_response:\n",
    "            code_response = code_response.split(stop)[0].strip()\n",
    "\n",
    "    return {\n",
    "        \"code\": code_response,\n",
    "        \"metrics\": {\n",
    "            \"generation_time\": generation_time,\n",
    "            \"input_tokens\": input_length,\n",
    "            \"output_tokens\": new_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_second\": new_tokens / generation_time if generation_time > 0 else 0,\n",
    "            \"memory_used_mb\": (memory_after - memory_before) / 1e6\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test basic code generation\n",
    "test_instruction = \"Create a Python function that implements binary search on a sorted list\"\n",
    "result = generate_code(test_instruction)\n",
    "\n",
    "print(\"üîß Code Generation Test:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"  Time: {result['metrics']['generation_time']:.2f}s\")\n",
    "print(f\"  Speed: {result['metrics']['tokens_per_second']:.1f} tok/s\")\n",
    "print(f\"  Tokens: {result['metrics']['output_tokens']}\")\n",
    "\n",
    "print(f\"\\nüíª Generated Code:\\n{result['code']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54f847-5424-4cfa-ae13-99f1c36f3091",
   "metadata": {},
   "source": [
    "# Optimized Code for Single H100 NVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bbd578-c7e1-4925-b4fb-249efb9dc630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.2.tar.gz (8.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from flash-attn) (2.7.1)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (0.6.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (12.6.85)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (4.14.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (11.7.1.2)\n",
      "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (3.18.0)\n",
      "Requirement already satisfied: triton==3.3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch->flash-attn) (10.3.7.77)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from triton==3.3.1->torch->flash-attn) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.0.1)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.8.2-cp310-cp310-linux_x86_64.whl size=255921566 sha256=7365feb5c5d54aa3838ea02f5fc52c51e496ec0db05e49907d6f1c152ad4e352\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/1b/95/ad/b6f5ede0a6d0fc052b5a5aebda836880f35caff6d51dd68fbf\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.1 flash-attn-2.8.2\n",
      "Requirement already satisfied: flash-attn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "# Install Flash Attention 2 for H100\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "# If the above fails, try with specific CUDA version\n",
    "!pip install flash-attn --no-build-isolation --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1407bc07-0500-45d9-a583-d276634320b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-Coder-32B-Instruct optimized for single H100 NVL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:09<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model for H100...\n",
      "‚úÖ Model loaded and compiled!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 66.60 GB\n",
      "üî• GPU Memory - Allocated: 66.60 GB\n",
      "üî• GPU Memory - Reserved: 66.61 GB\n",
      "üî• GPU Memory - Total: 99.87 GB\n",
      "üî• GPU Memory - Free: 33.27 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "# Set environment variables for H100 optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "custom_cache_dir = \"/dev/shm\"\n",
    "\n",
    "print(f\"Loading {model_name} optimized for single H100 NVL...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# H100 Optimized Loading - WORKING VERSION\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,  # BF16 is optimal for H100\n",
    "    device_map=\"cuda:0\",  # Single GPU\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=custom_cache_dir,\n",
    "    # REMOVED: attn_implementation=\"flash_attention_2\"  # This was causing the error\n",
    "    use_cache=True,\n",
    "    # Memory optimization for single GPU\n",
    "    max_memory={\"cuda:0\": \"85GB\"},  # Leave ~10GB for activations and KV cache\n",
    ")\n",
    "\n",
    "# H100 Performance optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "\n",
    "print(\"Compiling model for H100...\")\n",
    "# Enable PyTorch 2.0 compilation for performance\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")  # More stable than max-autotune\n",
    "\n",
    "print(f\"‚úÖ Model loaded and compiled!\")\n",
    "print(f\"üìç Device: {model.device}\")\n",
    "print(f\"üíæ Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "# Check GPU utilization\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üî• GPU Memory - Allocated: {gpu_memory:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Reserved: {gpu_reserved:.2f} GB\") \n",
    "    print(f\"üî• GPU Memory - Total: {gpu_total:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Free: {(gpu_total - gpu_reserved):.2f} GB\")\n",
    "\n",
    "# Optimized generation function\n",
    "def generate_h100_optimized(prompt, max_new_tokens=1024, temperature=0.7):\n",
    "    \"\"\"Optimized generation for H100 NVL\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dca455c-f8ac-4102-a829-aa0775f5a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing both generation methods...\n",
      "================================================================================\n",
      "üî¨ GENERATION METHOD COMPARISON\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Testing BASIC generation method...\n",
      "\n",
      "üìä BASIC GENERATION METRICS:\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 21.79s\n",
      "üíæ Memory delta: 0.034 GB\n",
      "üìù Generated tokens: 537\n",
      "‚ö° Tokens/second: 24.6\n",
      "\n",
      "2Ô∏è‚É£ Testing OPTIMIZED generation method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä OPTIMIZED GENERATION METRICS:\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 13.59s\n",
      "üíæ Memory delta: 0.000 GB\n",
      "üìù Generated tokens: 368\n",
      "‚ö° Tokens/second: 27.1\n",
      "\n",
      "================================================================================\n",
      "üìä COMPARISON COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìÑ BASIC Generated text (first 200 chars):\n",
      "Kubernetes is a powerful open-source platform designed to automate deploying, scaling, and operating application containers. While it's primarily used for web applications, it can also be beneficial for machine learning (ML) workloads. Here are some reasons why:\n",
      "\n",
      "1. **Resource Management**: Kubernetes efficiently manages resources by automatically allocating and deallocating them based on workload requirements. This is crucial for ML workloads, which can be resource-intensive and require significant computational power.\n",
      "\n",
      "2. **Scalability**: Kubernetes provides horizontal scalability, meaning it can scale up or down the number of pods (containers) based on demand. This is particularly useful for ML workloads that can vary in size and complexity, such as training large models or processing big data sets.\n",
      "\n",
      "3. **Self-Healing**: Kubernetes has built-in self-healing capabilities that ensure your ML workloads run smoothly. It can automatically restart failed containers, replace unhealthy nodes, and reschedule tasks to maintain availability and performance.\n",
      "\n",
      "4. **Portability**: Kubernetes makes it easy to deploy ML workloads across different environments, including on-premises, cloud, and hybrid setups. This flexibility allows you to choose the best infrastructure for your specific needs.\n",
      "\n",
      "5. **Security**: Kubernetes offers robust security features, such as network policies, role-based access control (RBAC), and secrets management, which help protect sensitive ML data and models.\n",
      "\n",
      "6. **Monitoring and Logging**: Kubernetes integrates with various monitoring and logging tools, making it easier to track the performance and health of your ML workloads. This visibility is essential for troubleshooting issues and optimizing performance.\n",
      "\n",
      "7. **Cost Efficiency**: By efficiently managing resources and providing scalability options, Kubernetes can help reduce costs associated with running ML workloads. It ensures that you only use the resources you need, avoiding unnecessary expenses.\n",
      "\n",
      "8. **Automation**: Kubernetes automates many aspects of deploying and managing ML workloads, freeing up your team to focus on more strategic tasks. This automation includes tasks like deployment, scaling, and updates.\n",
      "\n",
      "9. **Community and Ecosystem**: Kubernetes has a large and active community, along with a rich ecosystem of tools and plugins that can enhance its functionality for ML workloads. This community support and tooling can accelerate development and deployment cycles.\n",
      "\n",
      "In summary, Kubernetes offers numerous benefits for machine learning workloads, including efficient resource management, scalability, self-healing, portability, security, monitoring, cost efficiency, automation, and a strong community ecosystem. These features make Kubernetes a valuable tool for organizations looking to optimize their ML operations. Kubernetes' ability to handle the dynamic and resource-intensive nature of ML workloads makes it an excellent choice for both small-scale experiments and large-scale production deployments....\n",
      "\n",
      "üìÑ OPTIMIZED Generated text (first 200 chars):\n",
      " Kubernetes is a powerful container orchestration platform that can be used to manage and deploy machine learning workloads. Here are some of the benefits of using Kubernetes for machine learning:\n",
      "\n",
      "1. Scalability: Kubernetes allows for easy scaling of machine learning workloads up or down based on demand. This means that you can handle varying loads without having to manually provision or deprovision resources.\n",
      "\n",
      "2. Resource Management: Kubernetes provides efficient resource management by automatically allocating resources to workloads based on their needs. This ensures that your machine learning models have the resources they need to run efficiently.\n",
      "\n",
      "3. Fault Tolerance: Kubernetes is designed to be fault-tolerant, which means that it can automatically recover from failures such as node failures or application crashes. This ensures that your machine learning workloads are always running and available.\n",
      "\n",
      "4. Portability: Kubernetes is a portable platform that can be used on-premises, in the cloud, or in a hybrid environment. This means that you can easily move your machine learning workloads between different environments without having to make significant changes to your code.\n",
      "\n",
      "5. Security: Kubernetes provides a range of security features such as network policies, RBAC (Role-Based Access Control), and secrets management. These features help to ensure that your machine learning workloads are secure and protected from unauthorized access.\n",
      "\n",
      "6. Integration with other tools: Kubernetes integrates well with a wide range of other tools and platforms, including popular machine learning frameworks such as TensorFlow and PyTorch. This makes it easy to build and deploy machine learning pipelines that leverage the best tools and technologies available.\n",
      "\n",
      "Overall, Kubernetes provides a powerful and flexible platform for managing and deploying machine learning workloads. Its scalability, resource management, fault tolerance, portability, security, and integration capabilities make it an ideal choice for organizations looking to build and deploy machine learning models at scale....\n",
      "\n",
      "‚úÖ Model comparison complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Basic generation function for comparison\n",
    "def generate_text(prompt, max_length=2000, temperature=0.7, top_p=0.8):\n",
    "    \"\"\"Basic text generation function\"\"\"\n",
    "    \n",
    "    # Encode input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    # Track GPU memory before generation\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    new_text = response[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\nüìä BASIC GENERATION METRICS:\")\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {len(outputs[0]) - len(inputs['input_ids'][0])}\")\n",
    "    print(f\"‚ö° Tokens/second: {(len(outputs[0]) - len(inputs['input_ids'][0])) / generation_time:.1f}\")\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "# Enhanced optimized generation function with metrics\n",
    "def generate_h100_optimized_with_metrics(prompt, max_new_tokens=2000, temperature=0.7):\n",
    "    \"\"\"Optimized generation for H100 NVL with performance metrics\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "    \n",
    "    # Track GPU memory before generation\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.inference_mode():  # More efficient than torch.no_grad()\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "    tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])\n",
    "    \n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä OPTIMIZED GENERATION METRICS:\")\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {tokens_generated}\")\n",
    "    print(f\"‚ö° Tokens/second: {tokens_generated / generation_time:.1f}\")\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Performance comparison function\n",
    "def compare_generation_methods(prompt, max_tokens=2000):\n",
    "    \"\"\"Compare basic vs optimized generation\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üî¨ GENERATION METHOD COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test basic generation\n",
    "    print(\"\\n1Ô∏è‚É£ Testing BASIC generation method...\")\n",
    "    torch.cuda.empty_cache()  # Clear cache for fair comparison\n",
    "    result_basic = generate_text(prompt, max_length=len(tokenizer(prompt)['input_ids']) + max_tokens)\n",
    "    \n",
    "    # Test optimized generation\n",
    "    print(\"\\n2Ô∏è‚É£ Testing OPTIMIZED generation method...\")\n",
    "    torch.cuda.empty_cache()  # Clear cache for fair comparison\n",
    "    result_optimized = generate_h100_optimized_with_metrics(prompt, max_new_tokens=max_tokens)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä COMPARISON COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return result_basic, result_optimized\n",
    "\n",
    "# Test both methods\n",
    "print(\"\\nüß™ Testing both generation methods...\")\n",
    "test_prompt = \"Explain the benefits of using Kubernetes for machine learning workloads:\"\n",
    "\n",
    "# Run comparison\n",
    "basic_result, optimized_result = compare_generation_methods(test_prompt, max_tokens=2000)\n",
    "\n",
    "print(f\"\\nüìÑ BASIC Generated text :\\n{basic_result}...\")\n",
    "print(f\"\\nüìÑ OPTIMIZED Generated text :\\n\\n\\n{optimized_result}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen LLM (Python 3.10)",
   "language": "python",
   "name": "qwen-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
