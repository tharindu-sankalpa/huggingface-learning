{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9329a5-64f5-46c0-8a19-5bc7e548e4dc",
   "metadata": {},
   "source": [
    "# üöÄ Qwen Model Benchmarking on GPU (H100, T4, L4)\n",
    "A hands-on exploration of Qwen LLM variants using Hugging Face Transformers and PyTorch with GPU performance metrics.\n",
    "\n",
    "\n",
    "## üß™ Environment Check: Python Setup\n",
    "Before starting any model benchmarking, it's important to verify the Python environment. This cell prints the path of the current Python executable and the Python version being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d8fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /anaconda/envs/qwen-llm/bin/python\n",
      "Python version: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff984a9",
   "metadata": {},
   "source": [
    "# System Resource Assessment\n",
    "Start by examining the available resources and verifying our environment setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ddc8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027286d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA H100 NVL', major=9, minor=0, total_memory=95248MB, multi_processor_count=132, uuid=bd8cf1f6-7f8f-efd8-6a5f-e28967475480, L2_cache_size=60MB)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6b8c77-27fa-41fb-a346-3669d0332f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA H100 NVL'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96ddfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0645bb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.692195892334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory().total/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ec70f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310.837890625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory().available/(1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1b580-9530-4f67-b011-a6391563d0b7",
   "metadata": {},
   "source": [
    "# Comprehensive System Assessment (NVIDIA H100 NVL)\n",
    "Start by examining the available resources and verifying our environment setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a0c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm-flash/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python executable: /anaconda/envs/qwen-llm-flash/bin/python\n",
      "üî• PyTorch version: 2.5.1+cu121\n",
      "ü§ñ Transformers version: 4.54.1\n",
      "üìÖ Benchmark timestamp: 2025-08-03T07:14:49.801516\n",
      "============================================================\n",
      "üéÆ GPU 0: NVIDIA H100 NVL\n",
      "   üíæ Memory: 93.02 GB\n",
      "   üîß Compute capability: 9.0\n",
      "üñ•Ô∏è  CPU cores: 40\n",
      "üß† Total RAM: 314.69 GB\n",
      "üíö Available RAM: 234.37 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Verify environment setup\n",
    "print(f\"üêç Python executable: {sys.executable}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers version: {__import__('transformers').__version__}\")\n",
    "print(f\"üìÖ Benchmark timestamp: {datetime.now().isoformat()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Comprehensive system resource assessment\"\"\"\n",
    "    info = {\n",
    "        \"cpu_cores\": psutil.cpu_count(),\n",
    "        \"total_ram_gb\": psutil.virtual_memory().total / (1024**3),\n",
    "        \"available_ram_gb\": psutil.virtual_memory().available / (1024**3),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        info[\"cuda_version\"] = torch.version.cuda\n",
    "        info[\"gpu_count\"] = torch.cuda.device_count()\n",
    "        info[\"gpus\"] = []\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            gpu_info = {\n",
    "                \"id\": i,\n",
    "                \"name\": gpu_props.name,\n",
    "                \"memory_gb\": gpu_props.total_memory / (1024**3),\n",
    "                \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\",\n",
    "                \"multiprocessor_count\": gpu_props.multi_processor_count\n",
    "            }\n",
    "            info[\"gpus\"].append(gpu_info)\n",
    "            print(f\"üéÆ GPU {i}: {gpu_props.name}\")\n",
    "            print(f\"   üíæ Memory: {gpu_props.total_memory / (1024**3):.2f} GB\")\n",
    "            print(f\"   üîß Compute capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available - check GPU drivers and PyTorch installation\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  CPU cores: {info['cpu_cores']}\")\n",
    "    print(f\"üß† Total RAM: {info['total_ram_gb']:.2f} GB\")\n",
    "    print(f\"üíö Available RAM: {info['available_ram_gb']:.2f} GB\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Get and store system information\n",
    "system_info = get_system_info()\n",
    "if system_info is None:\n",
    "    print(\"\\nüö® GPU setup required before continuing. Check environment configuration.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90a68f-ec7e-4ed8-8c03-3cfe92bf972c",
   "metadata": {},
   "source": [
    "# Comprehensive System Assessment (Nvidia Tesla T4)\n",
    "Start by examining the available resources and verifying our environment setup: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df30f744-4b28-49cf-b203-b50095146255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python executable: /anaconda/envs/qwen-llm/bin/python\n",
      "üî• PyTorch version: 2.7.1+cu118\n",
      "ü§ñ Transformers version: 4.44.0\n",
      "üìÖ Benchmark timestamp: 2025-08-02T17:32:55.451092\n",
      "============================================================\n",
      "üéÆ GPU 0: Tesla T4\n",
      "   üíæ Memory: 15.57 GB\n",
      "   üîß Compute capability: 7.5\n",
      "üñ•Ô∏è  CPU cores: 8\n",
      "üß† Total RAM: 54.92 GB\n",
      "üíö Available RAM: 53.07 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Verify environment setup\n",
    "print(f\"üêç Python executable: {sys.executable}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers version: {__import__('transformers').__version__}\")\n",
    "print(f\"üìÖ Benchmark timestamp: {datetime.now().isoformat()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Comprehensive system resource assessment\"\"\"\n",
    "    info = {\n",
    "        \"cpu_cores\": psutil.cpu_count(),\n",
    "        \"total_ram_gb\": psutil.virtual_memory().total / (1024**3),\n",
    "        \"available_ram_gb\": psutil.virtual_memory().available / (1024**3),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        info[\"cuda_version\"] = torch.version.cuda\n",
    "        info[\"gpu_count\"] = torch.cuda.device_count()\n",
    "        info[\"gpus\"] = []\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            gpu_info = {\n",
    "                \"id\": i,\n",
    "                \"name\": gpu_props.name,\n",
    "                \"memory_gb\": gpu_props.total_memory / (1024**3),\n",
    "                \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\",\n",
    "                \"multiprocessor_count\": gpu_props.multi_processor_count\n",
    "            }\n",
    "            info[\"gpus\"].append(gpu_info)\n",
    "            print(f\"üéÆ GPU {i}: {gpu_props.name}\")\n",
    "            print(f\"   üíæ Memory: {gpu_props.total_memory / (1024**3):.2f} GB\")\n",
    "            print(f\"   üîß Compute capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available - check GPU drivers and PyTorch installation\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  CPU cores: {info['cpu_cores']}\")\n",
    "    print(f\"üß† Total RAM: {info['total_ram_gb']:.2f} GB\")\n",
    "    print(f\"üíö Available RAM: {info['available_ram_gb']:.2f} GB\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Get and store system information\n",
    "system_info = get_system_info()\n",
    "if system_info is None:\n",
    "    print(\"\\nüö® GPU setup required before continuing. Check environment configuration.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466f1c3f-3a9e-4e61-9129-454d013d7466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  2 14:55:05 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 NVL                On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              62W / 400W |      3MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651a8b2d-143f-45d5-a2b9-6e9b566e0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  2 17:32:34 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   36C    P8              10W /  70W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9f764",
   "metadata": {},
   "source": [
    "# Troubleshooting Common Issues\n",
    "If you encounter CUDA-related errors, verify these common issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e99782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Troubleshooting GPU Setup ===\n",
      "‚úÖ PyTorch imported successfully\n",
      "   CUDA compiled version: 11.8\n",
      "   CUDA runtime version: (9, 0)\n",
      "‚úÖ GPU accessible: NVIDIA H100 NVL\n",
      "‚úÖ GPU tensor operations working\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common troubleshooting checks\n",
    "def troubleshoot_setup():\n",
    "    print(\"=== Troubleshooting GPU Setup ===\")\n",
    "    \n",
    "    # Check CUDA installation\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"‚úÖ PyTorch imported successfully\")\n",
    "        print(f\"   CUDA compiled version: {torch.version.cuda}\")\n",
    "        print(f\"   CUDA runtime version: {torch.cuda.get_device_capability()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PyTorch import error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check GPU accessibility\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        print(f\"‚úÖ GPU accessible: {torch.cuda.get_device_name(device)}\")\n",
    "        \n",
    "        # Test basic tensor operations\n",
    "        try:\n",
    "            test_tensor = torch.randn(1000, 1000).cuda()\n",
    "            result = torch.mm(test_tensor, test_tensor.t())\n",
    "            print(f\"‚úÖ GPU tensor operations working\")\n",
    "            del test_tensor, result  # Free memory\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå GPU tensor operations failed: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run troubleshooting if needed\n",
    "troubleshoot_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc801ab",
   "metadata": {},
   "source": [
    "# Loading and Testing Qwen2.5-7B-Instruct\n",
    "Now that our environment is verified, let's load the model and test its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ad2f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-Coder-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:08<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 65.53 GB\n",
      "üî• GPU Memory - Allocated: 65.53 GB, Reserved: 67.09 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model with T4-optimized settings\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "custom_cache_dir = \"/dev/shm\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with T4 GPU optimization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìç Device: {model.device}\")\n",
    "print(f\"üíæ Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "# Check GPU memory usage after loading\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"üî• GPU Memory - Allocated: {gpu_memory:.2f} GB, Reserved: {gpu_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211fa1ff-a52d-455e-a7ca-d2f894d37c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  3 03:47:10 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 NVL                On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              95W / 400W |  64595MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4125      C   /anaconda/envs/qwen-llm/bin/python        64586MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca7ab61-b911-437e-944a-c510dd0c827a",
   "metadata": {},
   "source": [
    "## ‚úÖ Recommended Strategy Between Model Loads\n",
    "To safely unload one model before loading the next, you should do this in between iterations in your loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3713816-c05d-4e80-85a7-b0925a307069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Unload previous model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6038d6b2-8511-42f5-ba9c-b7baeeef92d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  3 03:50:11 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 NVL                On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              96W / 400W |    613MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4125      C   /anaconda/envs/qwen-llm/bin/python          604MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5849bdd",
   "metadata": {},
   "source": [
    "# Basic Text Generation Testing\n",
    "Let's test the model with simple text generation to ensure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227a2b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 28.66s\n",
      "üíæ Memory delta: 0.034 GB\n",
      "üìù Generated tokens: 829\n",
      "‚ö° Tokens/second: 28.9\n",
      "\n",
      "üìÑ Generated text:\n",
      "Kubernetes is a powerful container orchestration platform that can be used to manage and deploy containerized applications at scale. While it is commonly used for web applications, it can also be used for machine learning workloads. Here are some benefits of using Kubernetes for machine learning workloads:\n",
      "\n",
      "1. Scalability: Kubernetes can handle large-scale machine learning workloads by automatically scaling up or down based on demand. This means that you can run your machine learning models on a cluster of machines without having to manually manage the infrastructure.\n",
      "\n",
      "2. Resource Management: Kubernetes provides fine-grained resource management, which means that you can allocate resources such as CPU and memory to specific machine learning workloads. This ensures that your workloads have the resources they need to run efficiently.\n",
      "\n",
      "3. Fault Tolerance: Kubernetes is designed to be fault-tolerant, which means that it can automatically recover from failures and ensure that your machine learning workloads continue to run smoothly even in the event of hardware failures.\n",
      "\n",
      "4. Portability: Kubernetes is a platform-agnostic solution, which means that you can deploy your machine learning workloads on any infrastructure, whether it's on-premises or in the cloud. This makes it easy to move your workloads between different environments as needed.\n",
      "\n",
      "5. Integration with other tools: Kubernetes integrates well with other tools and technologies commonly used in machine learning, such as TensorFlow, PyTorch, and Jupyter notebooks. This makes it easier to build and deploy machine learning pipelines.\n",
      "\n",
      "Overall, Kubernetes provides a robust and flexible platform for managing and deploying machine learning workloads at scale. Its scalability, resource management, fault tolerance, portability, and integration with other tools make it an ideal choice for organizations looking to build and deploy machine learning models at scale. Can you provide more details on how Kubernetes can help with distributed training of machine learning models? Certainly! Distributed training of machine learning models can be a complex and resource-intensive process, but Kubernetes can help simplify this process in several ways:\n",
      "\n",
      "1. **Resource Allocation**: Kubernetes allows you to allocate resources (CPU, memory, GPU, etc.) to individual pods (containers) running your machine learning tasks. This ensures that each task has the necessary resources to perform efficiently, which is crucial for distributed training where multiple nodes need to communicate and share data.\n",
      "\n",
      "2. **Scaling**: Kubernetes can automatically scale the number of pods based on demand or predefined rules. This is particularly useful during distributed training when you might need to add more nodes to speed up the training process or reduce it when the workload decreases.\n",
      "\n",
      "3. **Load Balancing**: Kubernetes can distribute incoming traffic across multiple pods, ensuring that no single pod becomes a bottleneck. This is important in distributed training where data needs to be processed and shared among multiple nodes.\n",
      "\n",
      "4. **Fault Tolerance**: Kubernetes can automatically restart failed pods and reschedule them on other nodes. This ensures that the training process continues uninterrupted even if one or more nodes fail.\n",
      "\n",
      "5. **Networking**: Kubernetes provides a robust networking model that facilitates communication between pods. This is essential for distributed training where nodes need to exchange data and gradients. Kubernetes can set up network policies and service discovery mechanisms to ensure smooth communication.\n",
      "\n",
      "6. **Storage Management**: Kubernetes supports various storage solutions, including persistent volumes, which can be used to store large datasets required for training. This ensures that data is available to all nodes participating in the distributed training process.\n",
      "\n",
      "7. **Integration with ML Frameworks**: Kubernetes integrates well with popular machine learning frameworks like TensorFlow, PyTorch, and Horovod, which are often used for distributed training. These frameworks can leverage Kubernetes to manage and orchestrate the training jobs efficiently.\n",
      "\n",
      "8. **Monitoring and Logging**: Kubernetes provides built-in monitoring and logging capabilities that help you track the performance and health of your distributed training jobs. Tools like Prometheus and Grafana can be integrated to provide real-time insights into resource usage, training progress, and potential issues.\n",
      "\n",
      "By leveraging these features, Kubernetes can significantly streamline the process of distributed training, making it more efficient, scalable, and reliable. This is particularly beneficial for organizations dealing with large-scale machine learning projects that require significant computational resources.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=200, temperature=0.7, top_p=0.8):\n",
    "    \"\"\"Basic text generation function\"\"\"\n",
    "    \n",
    "    # Encode input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    # Track GPU memory before generation\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    new_text = response[len(prompt):].strip()\n",
    "\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {len(outputs[0]) - len(inputs[0])}\")\n",
    "    print(f\"‚ö° Tokens/second: {(len(outputs[0]) - len(inputs[0])) / generation_time:.1f}\")\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "# Test basic generation\n",
    "test_prompt = \"Explain the benefits of using Kubernetes for machine learning workloads:\"\n",
    "result = generate_text(test_prompt, max_length=2000)\n",
    "print(f\"\\nüìÑ Generated text:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6270e08-332e-41a5-bfe1-ba17db095a9b",
   "metadata": {},
   "source": [
    "# üöÄ Model Benchmarking Framework\n",
    "\n",
    "## Overview\n",
    "\n",
    "This framework provides a **comprehensive and flexible setup** for benchmarking large language models (LLMs) across different:\n",
    "\n",
    "* ‚úÖ **Model variants** (e.g., base vs. instruct, 7B vs. 32B)\n",
    "* ‚úÖ **Hardware setups** (e.g., NVIDIA T4, A100, etc.)\n",
    "* ‚úÖ **Performance metrics** (e.g., GPU memory usage, latency, throughput)\n",
    "\n",
    "Its primary goal is to help you **evaluate and compare model behavior and efficiency** in different inference environments.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Features\n",
    "\n",
    "* üîÅ **Plug-and-play model loading** with Hugging Face `transformers`\n",
    "* üß† **Automatic device mapping** for multi-GPU compatibility\n",
    "* üïí **Precise generation timing**\n",
    "* üìä **Memory usage tracking** (allocated vs. reserved)\n",
    "* üìà **Tokens/sec throughput reporting**\n",
    "* üìÇ **Cache control** via custom directory (e.g., `/dev/shm` for RAM disk)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Usage Example\n",
    "\n",
    "```python\n",
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Components\n",
    "\n",
    "| Component              | Description                                                                    |\n",
    "| ---------------------- | ------------------------------------------------------------------------------ |\n",
    "| `load_and_generate()`  | Unified function to load model, generate output, and benchmark runtime metrics |\n",
    "| `AutoTokenizer`        | Dynamically loads tokenizer with optional padding fallback                     |\n",
    "| `AutoModelForCausalLM` | Loads the model using half-precision (FP16) and maps to available GPUs         |\n",
    "| GPU Tracker            | Measures memory delta before/after inference                                   |\n",
    "| Metrics Output         | Includes generation time, memory usage, token throughput                       |\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Notes\n",
    "\n",
    "* For best performance on **NVIDIA T4**, use FP16 models and limit max sequence lengths.\n",
    "* Use `/dev/shm` as `cache_dir` to improve disk I/O speed on Linux systems.\n",
    "* Consider adding `torch.cuda.reset_peak_memory_stats()` and `torch.cuda.max_memory_allocated()` if peak memory stats are needed.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this exported as a `.md` file or documented in a Jupyter notebook with visual charts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3f0202-48f4-4eac-ac12-2bb08c85f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_and_generate(model_name, prompt, max_length=200, temperature=0.7, top_p=0.8, cache_dir=\"/dev/shm\"):\n",
    "    \"\"\"\n",
    "    Load a model and tokenizer, then generate text from a prompt.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): Hugging Face model identifier\n",
    "        prompt (str): Input prompt for text generation\n",
    "        max_length (int): Maximum number of tokens in output\n",
    "        temperature (float): Sampling temperature\n",
    "        top_p (float): Nucleus sampling threshold\n",
    "        cache_dir (str): Directory to cache model and tokenizer\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\nüöÄ Loading model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìç Device: {model.device}\")\n",
    "    print(f\"üíæ Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"üî• GPU Memory - Allocated: {gpu_memory:.2f} GB, Reserved: {gpu_reserved:.2f} GB\")\n",
    "\n",
    "    # Encode input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    new_text = response[len(prompt):].strip()\n",
    "\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {len(outputs[0]) - len(inputs['input_ids'][0])}\")\n",
    "    print(f\"‚ö° Tokens/second: {(len(outputs[0]) - len(inputs['input_ids'][0])) / generation_time:.1f}\")\n",
    "\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c2b93-a272-4092-9fe7-2d41ddd7bb99",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder-7B-Instruct (NVIDIA H100 NVL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf433579-4fe3-41a1-912a-0e969f4e1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Loading model: Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 15.70 GB\n",
      "üî• GPU Memory - Allocated: 82.41 GB, Reserved: 82.65 GB\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "üïí Generation time: 5.39s\n",
      "üíæ Memory delta: 0.000 GB\n",
      "üìù Generated tokens: 311\n",
      "‚ö° Tokens/second: 57.7\n"
     ]
    }
   ],
   "source": [
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc5c62f-ca7d-4673-a5b5-b2660da9b4a6",
   "metadata": {},
   "source": [
    "# Upgrade transformers\n",
    "The Qwen3 models exists and requires transformers >= 4.51.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c115bb5-69d4-493a-bec3-7772a339c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list\n",
    "!conda activate qwen-llm\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43774a0f-35f3-42a7-8d89-c31d9b44f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.54.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552cf05a-d039-42de-90f2-95537e4a8ae3",
   "metadata": {},
   "source": [
    "# Qwen/Qwen3-30B-A3B-Thinking-2507 (NVIDIA H100 NVL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ba4d8-fefc-4994-9bf0-f055f88286cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Loading model: Qwen/Qwen3-30B-A3B-Thinking-2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:50<00:00,  6.92s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:16<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 61.06 GB\n",
      "üî• GPU Memory - Allocated: 61.14 GB, Reserved: 61.71 GB\n"
     ]
    }
   ],
   "source": [
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen3-30B-A3B-Thinking-2507\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782db3a3-c9ee-4e72-b539-11cd5e388591",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder-7B-Instruct (NVIDIA Tesla T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2c631-f114-4193-90f5-22fd0599f1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Loading model: Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:54<00:00, 13.69s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.02it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 15.70 GB\n",
      "üî• GPU Memory - Allocated: 13.72 GB, Reserved: 13.96 GB\n"
     ]
    }
   ],
   "source": [
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d590d-7ec1-4b24-b83f-21a1f6ec3f1f",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder-32B-Instruct (NVIDIA H100 NVL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a176017-86b4-4677-8384-f3df5e81ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Loading model: Qwen/Qwen2.5-Coder-32B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:08<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 66.60 GB\n",
      "üî• GPU Memory - Allocated: 66.60 GB, Reserved: 66.87 GB\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 16.62s\n",
      "üíæ Memory delta: 0.034 GB\n",
      "üìù Generated tokens: 402\n",
      "‚ö° Tokens/second: 24.2\n"
     ]
    }
   ],
   "source": [
    "output = load_and_generate(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    prompt=\"Explain the benefits of using Kubernetes for machine learning workloads:\",\n",
    "    max_length=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7ea28",
   "metadata": {},
   "source": [
    "# Chat Completion Format Testing\n",
    "Since we're building an API that mimics OpenAI's chat completion format, let's test the model with proper chat formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc80439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing chat completion format...\n",
      "\n",
      "üìã Performance Metrics:\n",
      "   ‚è±Ô∏è  Generation time: 28.54s\n",
      "   üì• Input tokens: 46\n",
      "   üì§ Output tokens: 680\n",
      "   ‚ö° Speed: 23.8 tokens/sec\n",
      "   üíæ Memory used: 33.6 MB\n",
      "\n",
      "ü§ñ Assistant Response:\n",
      "Deploying Large Language Models (LLMs) on Kubernetes offers several key advantages over traditional VM-based deployments, particularly in the context of scalability, efficiency, and operational flexibility. Here are some of the primary benefits:\n",
      "\n",
      "1. **Scalability**:\n",
      "   - **Horizontal Scaling**: Kubernetes allows for easy horizontal scaling of LLM workloads. You can scale up or down the number of pods (instances) running your model based on demand, which is crucial for handling variable loads typical in applications using LLMs.\n",
      "   - **Auto-scaling**: With Kubernetes, you can set up auto-scaling policies that automatically adjust the number of replicas of your application based on CPU/memory usage or custom metrics, ensuring optimal resource utilization.\n",
      "\n",
      "2. **Resource Efficiency**:\n",
      "   - **Efficient Resource Allocation**: Kubernetes uses containers, which are lightweight and share the host OS kernel, allowing for more efficient use of resources compared to full VMs. This means you can run more instances of your LLM on the same hardware.\n",
      "   - **Fine-grained Resource Management**: You can specify precise CPU and memory requirements for each container, optimizing resource allocation and reducing costs.\n",
      "\n",
      "3. **Operational Flexibility**:\n",
      "   - **Declarative Configuration**: Kubernetes uses declarative configuration files (YAML) to define the desired state of your application, making it easier to manage and replicate environments across different stages (development, testing, production).\n",
      "   - **Continuous Integration/Continuous Deployment (CI/CD)**: Kubernetes integrates well with CI/CD pipelines, allowing for rapid deployment and updates of your LLM applications with minimal downtime.\n",
      "\n",
      "4. **High Availability and Fault Tolerance**:\n",
      "   - **Self-healing**: Kubernetes automatically restarts failed containers, replaces and reschedules them on healthy nodes, and kills containers that don‚Äôt respond to user-defined health checks.\n",
      "   - **Load Balancing**: Kubernetes provides built-in load balancing to distribute traffic across multiple instances of your application, improving availability and reliability.\n",
      "\n",
      "5. **Security**:\n",
      "   - **Isolation**: Containers provide better isolation between different applications and services, reducing the risk of security breaches.\n",
      "   - **Network Policies**: Kubernetes supports network policies to control traffic flow at the IP address or port level, enhancing security.\n",
      "\n",
      "6. **Monitoring and Logging**:\n",
      "   - **Built-in Monitoring**: Tools like Prometheus and Grafana can be integrated with Kubernetes to monitor application performance and resource usage in real-time.\n",
      "   - **Centralized Logging**: Kubernetes supports centralized logging solutions like ELK Stack or Fluentd, making it easier to aggregate and analyze logs from all containers.\n",
      "\n",
      "7. **Cost-Effectiveness**:\n",
      "   - **Pay-as-you-go Model**: By efficiently utilizing resources and scaling only when necessary, Kubernetes can help reduce cloud infrastructure costs.\n",
      "   - **Spot Instances**: Kubernetes can be configured to use spot instances, which are significantly cheaper than on-demand instances, providing a cost-effective way to run LLMs.\n",
      "\n",
      "8. **Ease of Use and Community Support**:\n",
      "   - **Extensive Documentation and Community**: Kubernetes has extensive documentation and a large community, making it easier to find support and best practices for deploying and managing LLMs.\n",
      "\n",
      "In summary, Kubernetes provides a robust, scalable, and flexible platform for deploying LLMs, offering significant advantages over traditional VM-based deployments in terms of resource management, operational efficiency, and cost-effectiveness.\n"
     ]
    }
   ],
   "source": [
    "def format_chat_prompt(messages):\n",
    "    \"\"\"Format messages for Qwen2.5-Instruct chat template\"\"\"\n",
    "    \n",
    "    # Qwen2.5-Instruct uses a specific chat format\n",
    "    formatted_prompt = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        \n",
    "        if role == \"system\":\n",
    "            formatted_prompt += f\"<|im_start|>system\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"user\":\n",
    "            formatted_prompt += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            formatted_prompt += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n",
    "    \n",
    "    # Add assistant start token for generation\n",
    "    formatted_prompt += \"<|im_start|>assistant\\n\"\n",
    "    return formatted_prompt\n",
    "\n",
    "def chat_completion(messages, max_tokens=500, temperature=0.7, top_p=0.8):\n",
    "    \"\"\"OpenAI-style chat completion function\"\"\"\n",
    "    \n",
    "    # Format the conversation\n",
    "    prompt = format_chat_prompt(messages)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = len(inputs[0])\n",
    "    \n",
    "    # Track performance metrics\n",
    "    start_time = time.time()\n",
    "    memory_before = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_tokens,  # Use max_new_tokens instead of max_length\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            stop_strings=[\"<|im_end|>\"],  # Stop at chat format end token\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated()\n",
    "    total_tokens = len(outputs[0])\n",
    "    new_tokens = total_tokens - input_length\n",
    "    \n",
    "    # Decode and clean response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_response = full_response[len(tokenizer.decode(inputs[0], skip_special_tokens=True)):].strip()\n",
    "    \n",
    "    # Remove chat format tokens if present\n",
    "    if assistant_response.endswith(\"<|im_end|>\"):\n",
    "        assistant_response = assistant_response[:-11].strip()\n",
    "    \n",
    "    # Performance metrics\n",
    "    tokens_per_second = new_tokens / generation_time if generation_time > 0 else 0\n",
    "    memory_used_mb = (memory_after - memory_before) / 1e6\n",
    "    \n",
    "    return {\n",
    "        \"response\": assistant_response,\n",
    "        \"metrics\": {\n",
    "            \"generation_time\": generation_time,\n",
    "            \"input_tokens\": input_length,\n",
    "            \"output_tokens\": new_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_second\": tokens_per_second,\n",
    "            \"memory_used_mb\": memory_used_mb\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test chat completion format\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a helpful AI assistant specializing in cloud infrastructure and DevOps.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What are the key advantages of deploying LLMs on Kubernetes compared to traditional VM-based deployments?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing chat completion format...\")\n",
    "result = chat_completion(test_messages, max_tokens=1000)\n",
    "\n",
    "print(f\"\\nüìã Performance Metrics:\")\n",
    "print(f\"   ‚è±Ô∏è  Generation time: {result['metrics']['generation_time']:.2f}s\")\n",
    "print(f\"   üì• Input tokens: {result['metrics']['input_tokens']}\")\n",
    "print(f\"   üì§ Output tokens: {result['metrics']['output_tokens']}\")\n",
    "print(f\"   ‚ö° Speed: {result['metrics']['tokens_per_second']:.1f} tokens/sec\")\n",
    "print(f\"   üíæ Memory used: {result['metrics']['memory_used_mb']:.1f} MB\")\n",
    "\n",
    "print(f\"\\nü§ñ Assistant Response:\\n{result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7cb3da",
   "metadata": {},
   "source": [
    "# Code Generation Testing Framework\n",
    "Let's create a comprehensive testing framework for coding capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Code Generation Test:\n",
      "==================================================\n",
      "Instruction: Create a Python function that implements binary search on a sorted list\n",
      "\n",
      "üìä Performance:\n",
      "  Time: 25.90s\n",
      "  Speed: 23.9 tok/s\n",
      "  Tokens: 618\n",
      "\n",
      "üíª Generated Code:\n",
      "Certainly! Below is a Python function that implements the binary search algorithm on a sorted list. The function returns the index of the target element if it is found in the list, and `-1` if the target is not present.\n",
      "\n",
      "```python\n",
      "def binary_search(sorted_list, target):\n",
      "    \"\"\"\n",
      "    Perform binary search on a sorted list to find the index of the target element.\n",
      "\n",
      "    Parameters:\n",
      "    sorted_list (list): A list of elements sorted in ascending order.\n",
      "    target: The element to search for in the list.\n",
      "\n",
      "    Returns:\n",
      "    int: The index of the target element if found, otherwise -1.\n",
      "    \"\"\"\n",
      "    left, right = 0, len(sorted_list) - 1\n",
      "\n",
      "    while left <= right:\n",
      "        mid = left + (right - left) // 2  # Calculate the middle index\n",
      "\n",
      "        # Check if the target is present at mid\n",
      "        if sorted_list[mid] == target:\n",
      "            return mid\n",
      "        # If target is greater, ignore the left half\n",
      "        elif sorted_list[mid] < target:\n",
      "            left = mid + 1\n",
      "        # If target is smaller, ignore the right half\n",
      "        else:\n",
      "            right = mid - 1\n",
      "\n",
      "    # Target is not present in the list\n",
      "    return -1\n",
      "\n",
      "# Example usage:\n",
      "if __name__ == \"__main__\":\n",
      "    sorted_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "    target = 7\n",
      "    result = binary_search(sorted_list, target)\n",
      "    \n",
      "    if result != -1:\n",
      "        print(f\"Element {target} is present at index {result}.\")\n",
      "    else:\n",
      "        print(f\"Element {target} is not present in the list.\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **Parameters**:\n",
      "  - `sorted_list`: A list of elements that must be sorted in ascending order.\n",
      "  - `target`: The element you want to find in the list.\n",
      "\n",
      "- **Returns**:\n",
      "  - The index of the `target` element if it is found in the list.\n",
      "  - `-1` if the `target` is not found in the list.\n",
      "\n",
      "- **Algorithm**:\n",
      "  - Initialize two pointers, `left` and `right`, to the start and end of the list, respectively.\n",
      "  - While `left` is less than or equal to `right`:\n",
      "    - Calculate the middle index `mid`.\n",
      "    - Compare the middle element with the target:\n",
      "      - If they are equal, return `mid`.\n",
      "      - If the middle element is less than the target, move the `left` pointer to `mid + 1`.\n",
      "      - If the middle element is greater than the target, move the `right` pointer to `mid - 1`.\n",
      "  - If the loop ends without finding the target, return `-1`.\n",
      "\n",
      "This implementation is efficient with a time complexity of \\(O(\\log n)\\), where \\(n\\) is the number of elements in the list.\n"
     ]
    }
   ],
   "source": [
    "def format_coding_prompt(instruction, code_context=\"\", language=\"python\"):\n",
    "    \"\"\"Format coding prompts for Qwen2.5-Coder\"\"\"\n",
    "    \n",
    "    system_message = (\n",
    "        \"You are a helpful coding assistant. You provide accurate, efficient, and well-documented code solutions.\\n\"\n",
    "        \"When writing code, follow these principles:\\n\"\n",
    "        \"1. Write clean, readable code with proper formatting\\n\"\n",
    "        \"2. Include helpful comments where necessary\\n\"\n",
    "        \"3. Follow language-specific best practices\\n\"\n",
    "        \"4. Provide complete, working solutions\"\n",
    "    )\n",
    "\n",
    "    if code_context:\n",
    "        user_message = f\"\"\"Language: {language}\n",
    "\n",
    "        Context:\n",
    "        ```{language}\n",
    "        {code_context}\n",
    "        ``` {{data-source-line=\"318\"}}\n",
    "\n",
    "        Request: {instruction}\"\"\"\n",
    "    else:\n",
    "        user_message = f\"\"\"Language: {language}\n",
    "\n",
    "        Request: {instruction}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    formatted_prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        formatted_prompt += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "\n",
    "    formatted_prompt += \"<|im_start|>assistant\\n\"\n",
    "    return formatted_prompt\n",
    "\n",
    "def generate_code(instruction, code_context=\"\", language=\"python\", max_tokens=2000, temperature=0.2):\n",
    "    \"\"\"Generate code with performance metrics\"\"\"\n",
    "    \n",
    "    prompt = format_coding_prompt(instruction, code_context, language)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    start_time = time.time()\n",
    "    memory_before = torch.cuda.memory_allocated()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            stop_strings=[\"<|im_end|>\", \"<|endoftext|>\"],\n",
    "            tokenizer=tokenizer  # ‚úÖ required when using stop_strings\n",
    "        )\n",
    "\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated()\n",
    "    total_tokens = outputs.shape[1]\n",
    "    new_tokens = total_tokens - input_length\n",
    "\n",
    "    decoded_prompt = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    code_response = decoded_output[len(decoded_prompt):].strip()\n",
    "\n",
    "    for stop in [\"<|im_end|>\", \"<|endoftext|>\"]:\n",
    "        if stop in code_response:\n",
    "            code_response = code_response.split(stop)[0].strip()\n",
    "\n",
    "    return {\n",
    "        \"code\": code_response,\n",
    "        \"metrics\": {\n",
    "            \"generation_time\": generation_time,\n",
    "            \"input_tokens\": input_length,\n",
    "            \"output_tokens\": new_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_second\": new_tokens / generation_time if generation_time > 0 else 0,\n",
    "            \"memory_used_mb\": (memory_after - memory_before) / 1e6\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test basic code generation\n",
    "test_instruction = \"Create a Python function that implements binary search on a sorted list\"\n",
    "result = generate_code(test_instruction)\n",
    "\n",
    "print(\"üîß Code Generation Test:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"  Time: {result['metrics']['generation_time']:.2f}s\")\n",
    "print(f\"  Speed: {result['metrics']['tokens_per_second']:.1f} tok/s\")\n",
    "print(f\"  Tokens: {result['metrics']['output_tokens']}\")\n",
    "\n",
    "print(f\"\\nüíª Generated Code:\\n{result['code']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7db78-9ba4-4f67-8be1-4de1b1c9b09f",
   "metadata": {},
   "source": [
    "## üîç Qwen Model Code Generation Benchmarking Framework\n",
    "\n",
    "To accurately assess how different Qwen model variants perform in generating production-grade code, we implemented a full evaluation harness in Python. This framework compares models in terms of:\n",
    "\n",
    "- ‚è±Ô∏è **Generation Time**\n",
    "- ‚ö° **Tokens per Second**\n",
    "- üíæ **GPU Memory Used**\n",
    "- üßæ **Complete Generated Output**\n",
    "\n",
    "We run the same real-world coding task prompt across multiple Qwen variants and save the complete code output for manual inspection and execution.\n",
    "\n",
    "### üì¶ Prompt Used\n",
    "\n",
    "```text\n",
    "I need to create a FastAPI server to serve the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection. The API should follow best practices and be optimized for high inference performance. Please provide the complete implementation.\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è GPU Memory Handling Between Model Loads\n",
    "\n",
    "Loading large models sequentially requires explicit memory cleanup between runs to avoid out-of-memory errors. Relying on Python‚Äôs function scoping or `torch.cuda.empty_cache()` alone is insufficient.\n",
    "\n",
    "We ensure proper cleanup by:\n",
    "- Deleting the model and tokenizer using `del`\n",
    "- Triggering garbage collection with `gc.collect()`\n",
    "- Emptying unused cached memory with `torch.cuda.empty_cache()`\n",
    "\n",
    "### üìÅ Output Structure\n",
    "\n",
    "Each model‚Äôs code output is saved to a timestamped `.txt` file, and a summary CSV logs all metrics:\n",
    "\n",
    "```\n",
    "qwen_codegen_outputs/\n",
    "‚îú‚îÄ‚îÄ Qwen2.5-Coder-7B-Instruct__20250803_121233.txt\n",
    "‚îú‚îÄ‚îÄ Qwen3-30B-A3B__20250803_121310.txt\n",
    "‚îú‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ qwen_codegen_summary.csv\n",
    "```\n",
    "\n",
    "You can open and test each `.txt` output manually for quality, correctness, or runtime behavior.\n",
    "\n",
    "### üß† Why This Matters\n",
    "\n",
    "This benchmarking framework is essential for:\n",
    "- Validating model behavior on real-world coding tasks\n",
    "- Selecting the right Qwen variant for production\n",
    "- Documenting performance trade-offs across 7B vs. 30B vs. 32B sizes\n",
    "\n",
    "Use this structure to inform deployment decisions and clearly demonstrate how well each model performs in actual DevOps-focused tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ef5ab7-7d1f-497d-9eb8-44b1cd6b9b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm-flash/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting H100-Optimized Qwen Code Generation Benchmark\n",
      "üìÅ Output directory: qwen_codegen_outputs-flash\n",
      "üî¢ Models to test: 5\n",
      "\n",
      "============================================================\n",
      "üîÑ Processing model 1/5: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "============================================================\n",
      "\n",
      "üîç Loading model: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "üß† GPU Memory before loading: 0.00GB allocated, 99.87GB free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:08<00:00, 32.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Flash Attention 2 enabled\n",
      "üîß Compiling model for H100...\n",
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 15.23 GB\n",
      "üî• GPU Memory - Allocated: 15.23 GB\n",
      "üî• GPU Memory - Reserved: 15.25 GB\n",
      "üî• GPU Memory - Total: 99.87 GB\n",
      "üî• GPU Memory - Free: 84.63 GB\n",
      "üó®Ô∏è  Using chat template\n",
      "üìù Input length: 81 tokens\n",
      "üéØ Generating up to 4096 new tokens...\n",
      "‚ö° Generated 550 tokens in 10.35s (53.14 tok/s)\n",
      "\n",
      "================================================================================\n",
      "GENERATED OUTPUT:\n",
      "================================================================================\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that provides complete, working code implementations.\n",
      "user\n",
      "I need to create a FastAPI server to serve the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection. The API should follow best practices and be optimized for high inference performance. Please provide the complete implementation.\n",
      "assistant\n",
      "To create a FastAPI server for serving the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection, you can follow these steps:\n",
      "\n",
      "1. Install the necessary dependencies:\n",
      "```bash\n",
      "pip install fastapi uvicorn transformers torch torchvision\n",
      "```\n",
      "\n",
      "2. Create a Python script for your FastAPI server:\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from fastapi.responses import JSONResponse\n",
      "from PIL import Image\n",
      "import requests\n",
      "from io import BytesIO\n",
      "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# Load the pre-trained model and feature extractor\n",
      "model_n...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Output saved to: qwen_codegen_outputs-flash/Qwen2.5-Coder-7B-Instruct__20250803_073223.md\n",
      "‚úÖ Successfully completed Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "\n",
      "============================================================\n",
      "üîÑ Processing model 2/5: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "============================================================\n",
      "\n",
      "üîç Loading model: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üß† GPU Memory before loading: 0.03GB allocated, 99.83GB free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [08:44<00:00, 37.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Flash Attention 2 enabled\n",
      "üîß Compiling model for H100...\n",
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 65.53 GB\n",
      "üî• GPU Memory - Allocated: 65.56 GB\n",
      "üî• GPU Memory - Reserved: 79.23 GB\n",
      "üî• GPU Memory - Total: 99.87 GB\n",
      "üî• GPU Memory - Free: 20.64 GB\n",
      "üó®Ô∏è  Using chat template\n",
      "üìù Input length: 81 tokens\n",
      "üéØ Generating up to 4096 new tokens...\n",
      "‚ö° Generated 800 tokens in 31.39s (25.49 tok/s)\n",
      "\n",
      "================================================================================\n",
      "GENERATED OUTPUT:\n",
      "================================================================================\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that provides complete, working code implementations.\n",
      "user\n",
      "I need to create a FastAPI server to serve the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection. The API should follow best practices and be optimized for high inference performance. Please provide the complete implementation.\n",
      "assistant\n",
      "Certainly! Below is a complete implementation of a FastAPI server that serves the Hugging Face Vision Transformer model `Falconsai/nsfw_image_detection`. This implementation includes best practices such as using dependency injection for model loading, handling image uploads, and optimizing inference performance.\n",
      "\n",
      "First, ensure you have the necessary packages installed:\n",
      "\n",
      "```bash\n",
      "pip install fastapi uvicorn transformers torch pillow\n",
      "```\n",
      "\n",
      "Now, here's the complete implementation:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
      "from fastapi.responses import JSONResponse\n",
      "from pydantic...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Output saved to: qwen_codegen_outputs-flash/Qwen2.5-Coder-32B-Instruct__20250803_074142.md\n",
      "‚úÖ Successfully completed Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "\n",
      "============================================================\n",
      "üîÑ Processing model 3/5: Qwen/Qwen3-32B\n",
      "============================================================\n",
      "\n",
      "üîç Loading model: Qwen/Qwen3-32B\n",
      "üß† GPU Memory before loading: 0.03GB allocated, 99.83GB free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [09:58<00:00, 35.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Flash Attention 2 enabled\n",
      "üîß Compiling model for H100...\n",
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 65.52 GB\n",
      "üî• GPU Memory - Allocated: 65.56 GB\n",
      "üî• GPU Memory - Reserved: 79.23 GB\n",
      "üî• GPU Memory - Total: 99.87 GB\n",
      "üî• GPU Memory - Free: 20.64 GB\n",
      "üìù Using direct prompt\n",
      "üìù Input length: 45 tokens\n",
      "üéØ Generating up to 4096 new tokens...\n",
      "‚ö° Generated 4096 tokens in 189.11s (21.66 tok/s)\n",
      "\n",
      "================================================================================\n",
      "GENERATED OUTPUT:\n",
      "================================================================================\n",
      "I need to create a FastAPI server to serve the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection. The API should follow best practices and be optimized for high inference performance. Please provide the complete implementation. Also, make sure that the API is secure by implementing rate limiting and authentication. Additionally, the API should support batch processing of images. Let me know if you need any clarifications.\n",
      "Okay, I need to create a FastAPI server for the HuggingFace model Falconsai/nsfw_image_detection. Let's start by understanding what this model does. It's an image classification model for NSFW content, right? So it probably takes images and returns labels like NSFW or safe.\n",
      "\n",
      "First, I'll need to install necessary packages. FastAPI is required, along with Uvicorn for running the server. For the model, I'll use transformers from HuggingFace. Also, for image processing, I might need PIL and torchvision. Oh, and for security, I'll need to handle authenti...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Output saved to: qwen_codegen_outputs-flash/Qwen3-32B__20250803_075452.md\n",
      "‚úÖ Successfully completed Qwen/Qwen3-32B\n",
      "\n",
      "============================================================\n",
      "üîÑ Processing model 4/5: Qwen/Qwen3-30B-A3B\n",
      "============================================================\n",
      "\n",
      "üîç Loading model: Qwen/Qwen3-30B-A3B\n",
      "üß† GPU Memory before loading: 0.03GB allocated, 99.83GB free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [09:37<00:00, 36.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Flash Attention 2 enabled\n",
      "üîß Compiling model for H100...\n",
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 61.06 GB\n",
      "üî• GPU Memory - Allocated: 61.10 GB\n",
      "üî• GPU Memory - Reserved: 75.73 GB\n",
      "üî• GPU Memory - Total: 99.87 GB\n",
      "üî• GPU Memory - Free: 24.14 GB\n",
      "üìù Using direct prompt\n",
      "üìù Input length: 45 tokens\n",
      "üéØ Generating up to 4096 new tokens...\n",
      "‚ö° Generated 4096 tokens in 385.83s (10.62 tok/s)\n",
      "\n",
      "================================================================================\n",
      "GENERATED OUTPUT:\n",
      "================================================================================\n",
      "I need to create a FastAPI server to serve the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection. The API should follow best practices and be optimized for high inference performance. Please provide the complete implementation. Also, I want to make sure that the model is loaded on the GPU if available.\n",
      "\n",
      "Okay, let's see. The user wants a FastAPI server to serve the HuggingFace Vision Transformer model for NSFW image detection. They mentioned using Falconsai/nsfw_image_detection, but wait, I should check if that's the correct model. Maybe they meant the NSFW image detection model from Hugging Face? Let me confirm. Oh right, there's a model called \"Falconsai/nsfw_image_detection\" which is a pre-trained model for detecting NSFW content. But I need to make sure about the exact name and how to load it.\n",
      "\n",
      "First, I need to set up the FastAPI application. Then, load the model and processor. Since the user wants GPU optimization, I'll have to ensure that the model is moved to t...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Output saved to: qwen_codegen_outputs-flash/Qwen3-30B-A3B__20250803_081100.md\n",
      "‚úÖ Successfully completed Qwen/Qwen3-30B-A3B\n",
      "\n",
      "============================================================\n",
      "üîÑ Processing model 5/5: Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "============================================================\n",
      "\n",
      "üîç Loading model: Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "üß† GPU Memory before loading: 0.03GB allocated, 99.83GB free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [13:46<00:00, 51.64s/it]  \n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [09:41<00:00, 36.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Flash Attention 2 enabled\n",
      "üîß Compiling model for H100...\n",
      "‚úÖ Model loaded successfully!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 61.06 GB\n",
      "üî• GPU Memory - Allocated: 61.10 GB\n",
      "üî• GPU Memory - Reserved: 75.73 GB\n",
      "üî• GPU Memory - Total: 99.87 GB\n",
      "üî• GPU Memory - Free: 24.14 GB\n",
      "üó®Ô∏è  Using chat template\n",
      "üìù Input length: 81 tokens\n",
      "üéØ Generating up to 4096 new tokens...\n",
      "‚ö° Generated 2275 tokens in 221.60s (10.27 tok/s)\n",
      "\n",
      "================================================================================\n",
      "GENERATED OUTPUT:\n",
      "================================================================================\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that provides complete, working code implementations.\n",
      "user\n",
      "I need to create a FastAPI server to serve the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection. The API should follow best practices and be optimized for high inference performance. Please provide the complete implementation.\n",
      "assistant\n",
      "Here's a complete FastAPI implementation for serving the HuggingFace NSFW image detection model with high performance optimizations:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "import logging\n",
      "from contextlib import asynccontextmanager\n",
      "from io import BytesIO\n",
      "from typing import List, Optional\n",
      "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from fastapi.responses import JSONResponse\n",
      "from pydantic import BaseModel\n",
      "from PIL import Image\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from concurrent.futures import ThreadP...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Output saved to: qwen_codegen_outputs-flash/Qwen3-Coder-30B-A3B-Instruct__20250803_083822.md\n",
      "‚úÖ Successfully completed Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "\n",
      "üéâ BENCHMARK COMPLETE!\n",
      "üìä Summary saved to: qwen_codegen_outputs-flash/qwen_codegen_summary.csv\n",
      "\n",
      "üìà RESULTS SUMMARY:\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Model                               | Flash |  Tokens |   Tok/s |    Time |  Model GB |  GPU Free | Status\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Qwen/Qwen2.5-Coder-7B-Instruct      |     ‚ö° |     550 |    53.1 |    10.3s |      15.2 |      84.6 | ‚úÖ OK\n",
      "Qwen/Qwen2.5-Coder-32B-Instruct     |     ‚ö° |     800 |    25.5 |    31.4s |      65.5 |      20.6 | ‚úÖ OK\n",
      "Qwen/Qwen3-32B                      |     ‚ö° |    4096 |    21.7 |   189.1s |      65.5 |      20.6 | ‚úÖ OK\n",
      "Qwen/Qwen3-30B-A3B                  |     ‚ö° |    4096 |    10.6 |   385.8s |      61.1 |      24.1 | ‚úÖ OK\n",
      "Qwen/Qwen3-Coder-30B-A3B-Instruct   |     ‚ö° |    2275 |    10.3 |   221.6s |      61.1 |      24.1 | ‚úÖ OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "MODELS = [\n",
    "    \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    \"Qwen/Qwen3-32B\",\n",
    "    \"Qwen/Qwen3-30B-A3B\",\n",
    "    \"Qwen/Qwen3-Coder-30B-A3B-Instruct\"\n",
    "]\n",
    "\n",
    "PROMPT = \"\"\"I need to create a FastAPI server to serve the HuggingFace Vision Transformer model, Falconsai/nsfw_image_detection. The API should follow best practices and be optimized for high inference performance. Please provide the complete implementation.\"\"\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"qwen_codegen_outputs-flash\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set H100 optimization environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_gpu_memory_stats():\n",
    "    \"\"\"Get comprehensive GPU memory statistics\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"allocated\": 0, \"reserved\": 0, \"total\": 0, \"free\": 0}\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    free = total - reserved\n",
    "    \n",
    "    return {\n",
    "        \"allocated\": allocated,\n",
    "        \"reserved\": reserved, \n",
    "        \"total\": total,\n",
    "        \"free\": free\n",
    "    }\n",
    "\n",
    "def format_prompt_for_qwen(prompt, model_name):\n",
    "    \"\"\"Format prompt appropriately for Qwen models\"\"\"\n",
    "    if \"Instruct\" in model_name or \"instruct\" in model_name:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant that provides complete, working code implementations.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        return messages\n",
    "    else:\n",
    "        return prompt\n",
    "\n",
    "def benchmark_model(model_name, prompt, temperature=0.7, top_p=0.9, max_new_tokens=4096, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/gpu-compute-h100/code/models\"):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nüîç Loading model: {model_name}\")\n",
    "    \n",
    "    # Pre-loading memory stats\n",
    "    memory_before_load = get_gpu_memory_stats()\n",
    "    print(f\"üß† GPU Memory before loading: {memory_before_load['allocated']:.2f}GB allocated, {memory_before_load['free']:.2f}GB free\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # H100 Optimized model loading\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,  # Optimal for H100\n",
    "            device_map=\"cuda:0\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            cache_dir=cache_dir,\n",
    "            attn_implementation=\"flash_attention_2\",  # H100 optimization\n",
    "            use_cache=True,\n",
    "            max_memory={\"cuda:0\": \"85GB\"}  # Leave headroom for generation\n",
    "        ).eval()\n",
    "        \n",
    "        flash_attention_used = True\n",
    "        print(\"‚ö° Flash Attention 2 enabled\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Flash Attention failed: {e}\")\n",
    "        print(\"üîÑ Falling back to standard attention...\")\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda:0\", \n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=True,\n",
    "            max_memory={\"cuda:0\": \"85GB\"}\n",
    "        ).eval()\n",
    "        \n",
    "        flash_attention_used = False\n",
    "    \n",
    "    # H100 Performance optimizations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "    \n",
    "    # Compile model for H100 optimization\n",
    "    print(\"üîß Compiling model for H100...\")\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    \n",
    "    # Post-loading memory stats\n",
    "    memory_after_load = get_gpu_memory_stats()\n",
    "    model_memory_gb = memory_after_load['allocated'] - memory_before_load['allocated']\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìç Device: {model.device}\")\n",
    "    print(f\"üíæ Model memory: {model_memory_gb:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Allocated: {memory_after_load['allocated']:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Reserved: {memory_after_load['reserved']:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Total: {memory_after_load['total']:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Free: {memory_after_load['free']:.2f} GB\")\n",
    "    \n",
    "    # Format prompt appropriately\n",
    "    formatted_prompt = format_prompt_for_qwen(prompt, model_name)\n",
    "    \n",
    "    # Prepare inputs based on model type\n",
    "    if isinstance(formatted_prompt, list):  # Chat messages\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            chat_input = tokenizer.apply_chat_template(\n",
    "                formatted_prompt, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            print(f\"üó®Ô∏è  Using chat template\")\n",
    "        else:\n",
    "            chat_input = f\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant that provides complete, working code implementations.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "            print(f\"üó®Ô∏è  Using manual chat formatting\")\n",
    "            \n",
    "        inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model.device)\n",
    "    else:\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        print(f\"üìù Using direct prompt\")\n",
    "    \n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    memory_before_gen = get_gpu_memory_stats()\n",
    "    \n",
    "    print(f\"üìù Input length: {input_len} tokens\")\n",
    "    print(f\"üéØ Generating up to {max_new_tokens} new tokens...\")\n",
    "    \n",
    "    # Get special tokens\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else eos_token_id\n",
    "    \n",
    "    gen_start = time.time()\n",
    "    \n",
    "    # H100 Optimized generation\n",
    "    with torch.inference_mode():  # More efficient than torch.no_grad()\n",
    "        try:\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                min_new_tokens=50,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                repetition_penalty=1.05,\n",
    "                early_stopping=False,\n",
    "                use_cache=True,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Generation failed: {e}\")\n",
    "            print(\"üîÑ Trying with simpler parameters...\")\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                min_new_tokens=20,\n",
    "                temperature=0.8,\n",
    "                do_sample=True,\n",
    "                pad_token_id=pad_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "                early_stopping=False,\n",
    "            )\n",
    "    \n",
    "    gen_end = time.time()\n",
    "    memory_after_gen = get_gpu_memory_stats()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_tokens = outputs.shape[1]\n",
    "    new_tokens = total_tokens - input_len\n",
    "    generation_time = gen_end - gen_start\n",
    "    \n",
    "    print(f\"‚ö° Generated {new_tokens} tokens in {generation_time:.2f}s ({new_tokens/generation_time:.2f} tok/s)\")\n",
    "    \n",
    "    # Decode output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED OUTPUT:\")\n",
    "    print(\"=\"*80)\n",
    "    print(decoded_output[:1000] + \"...\" if len(decoded_output) > 1000 else decoded_output)\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Extract only the generated part\n",
    "    if isinstance(formatted_prompt, list):\n",
    "        if \"<|im_start|>assistant\\n\" in decoded_output:\n",
    "            generated_code = decoded_output.split(\"<|im_start|>assistant\\n\", 1)[1]\n",
    "            if \"<|im_end|>\" in generated_code:\n",
    "                generated_code = generated_code.split(\"<|im_end|>\", 1)[0]\n",
    "        else:\n",
    "            generated_code = decoded_output[len(prompt):].strip()\n",
    "    else:\n",
    "        generated_code = decoded_output[len(prompt):].strip()\n",
    "    \n",
    "    generated_code = generated_code.strip()\n",
    "    \n",
    "    # Save to file\n",
    "    model_slug = model_name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = OUTPUT_DIR / f\"{model_slug}__{timestamp}.md\"\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== PROMPT ===\\n\")\n",
    "        f.write(prompt)\n",
    "        f.write(\"\\n\\n=== FULL OUTPUT ===\\n\")\n",
    "        f.write(decoded_output)\n",
    "        f.write(f\"\\n\\n=== STATS ===\\n\")\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Flash Attention: {flash_attention_used}\\n\")\n",
    "        f.write(f\"Generation time: {generation_time:.2f}s\\n\")\n",
    "        f.write(f\"Input tokens: {input_len}\\n\")\n",
    "        f.write(f\"Output tokens: {new_tokens}\\n\")\n",
    "        f.write(f\"Tokens per second: {new_tokens/generation_time:.2f}\\n\")\n",
    "        f.write(f\"Model memory: {model_memory_gb:.2f} GB\\n\")\n",
    "        f.write(f\"GPU total: {memory_after_load['total']:.2f} GB\\n\")\n",
    "        f.write(f\"GPU allocated: {memory_after_load['allocated']:.2f} GB\\n\")\n",
    "        f.write(f\"GPU reserved: {memory_after_load['reserved']:.2f} GB\\n\")\n",
    "        f.write(f\"GPU free: {memory_after_load['free']:.2f} GB\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Output saved to: {filename}\")\n",
    "    \n",
    "    result = {\n",
    "        \"model\": model_name,\n",
    "        \"output_file\": str(filename),\n",
    "        \"flash_attention\": flash_attention_used,\n",
    "        \"generation_time_sec\": generation_time,\n",
    "        \"input_tokens\": input_len,\n",
    "        \"output_tokens\": new_tokens,\n",
    "        \"tokens_per_second\": new_tokens / generation_time if generation_time > 0 else 0,\n",
    "        \"generated_chars\": len(generated_code),\n",
    "        \"model_memory_gb\": model_memory_gb,\n",
    "        \"gpu_total_gb\": memory_after_load['total'],\n",
    "        \"gpu_allocated_gb\": memory_after_load['allocated'],\n",
    "        \"gpu_reserved_gb\": memory_after_load['reserved'],\n",
    "        \"gpu_free_gb\": memory_after_load['free'],\n",
    "        \"total_tokens\": total_tokens\n",
    "    }\n",
    "    \n",
    "    # Cleanup GPU memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def run_all():\n",
    "    results = []\n",
    "    \n",
    "    print(\"üöÄ Starting H100-Optimized Qwen Code Generation Benchmark\")\n",
    "    print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"üî¢ Models to test: {len(MODELS)}\")\n",
    "    \n",
    "    for i, model in enumerate(MODELS, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîÑ Processing model {i}/{len(MODELS)}: {model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            result = benchmark_model(model, PROMPT)\n",
    "            results.append(result)\n",
    "            print(f\"‚úÖ Successfully completed {model}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {model}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"output_file\": None,\n",
    "                \"flash_attention\": False,\n",
    "                \"generation_time_sec\": None,\n",
    "                \"input_tokens\": None,\n",
    "                \"output_tokens\": None,\n",
    "                \"tokens_per_second\": None,\n",
    "                \"generated_chars\": None,\n",
    "                \"model_memory_gb\": None,\n",
    "                \"gpu_total_gb\": None,\n",
    "                \"gpu_allocated_gb\": None,\n",
    "                \"gpu_reserved_gb\": None,\n",
    "                \"gpu_free_gb\": None,\n",
    "                \"total_tokens\": None,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    summary_file = OUTPUT_DIR / \"qwen_codegen_summary.csv\"\n",
    "    df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüéâ BENCHMARK COMPLETE!\")\n",
    "    print(f\"üìä Summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nüìà RESULTS SUMMARY:\")\n",
    "    print(\"-\" * 150)\n",
    "    print(f\"{'Model':<35} | {'Flash':>5} | {'Tokens':>7} | {'Tok/s':>7} | {'Time':>7} | {'Model GB':>9} | {'GPU Free':>9} | {'Status'}\")\n",
    "    print(\"-\" * 150)\n",
    "    for _, row in df.iterrows():\n",
    "        if row['output_tokens'] is not None:\n",
    "            flash_icon = \"‚ö°\" if row['flash_attention'] else \"‚ùå\"\n",
    "            print(f\"{row['model']:<35} | {flash_icon:>5} | {row['output_tokens']:>7} | {row['tokens_per_second']:>7.1f} | {row['generation_time_sec']:>7.1f}s | {row['model_memory_gb']:>9.1f} | {row['gpu_free_gb']:>9.1f} | {'‚úÖ OK'}\")\n",
    "        else:\n",
    "            error_msg = str(row.get('error', 'Unknown'))[:15] + \"...\" if len(str(row.get('error', 'Unknown'))) > 15 else str(row.get('error', 'Unknown'))\n",
    "            print(f\"{row['model']:<35} | {'‚ùå':>5} | {'ERROR':>7} | {'N/A':>7} | {'N/A':>7} | {'N/A':>9} | {'N/A':>9} | ‚ùå {error_msg}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54f847-5424-4cfa-ae13-99f1c36f3091",
   "metadata": {},
   "source": [
    "# Optimized Code for Single H100 NVL\n",
    "\n",
    "Once the model is benchmarked and functional, production deployment requires pushing inference efficiency to its peak. In this section, we focus on optimizing Qwen2.5-Coder-32B-Instruct for the NVIDIA H100 NVL, leveraging cutting-edge hardware features such as BF16 precision, PyTorch 2.0 compilation, and FlashAttention 2 to dramatically improve speed, memory utilization, and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1407bc07-0500-45d9-a583-d276634320b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/qwen-llm-flash/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-Coder-32B-Instruct optimized for single H100 NVL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:08<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model for H100...\n",
      "‚úÖ Model loaded and compiled!\n",
      "üìç Device: cuda:0\n",
      "üíæ Model memory: 65.53 GB\n",
      "üî• GPU Memory - Allocated: 65.53 GB\n",
      "üî• GPU Memory - Reserved: 65.54 GB\n",
      "üî• GPU Memory - Total: 99.87 GB\n",
      "üî• GPU Memory - Free: 34.33 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "# Set environment variables for H100 optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "custom_cache_dir = \"/dev/shm\"\n",
    "\n",
    "print(f\"Loading {model_name} optimized for single H100 NVL...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=custom_cache_dir\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# H100 Optimized Loading - WORKING VERSION\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,  # BF16 is optimal for H100\n",
    "    device_map=\"cuda:0\",  # Single GPU\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=custom_cache_dir,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_cache=True,\n",
    "    # Memory optimization for single GPU\n",
    "    max_memory={\"cuda:0\": \"85GB\"},  # Leave ~10GB for activations and KV cache\n",
    ")\n",
    "\n",
    "# H100 Performance optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "\n",
    "print(\"Compiling model for H100...\")\n",
    "# Enable PyTorch 2.0 compilation for performance\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")  # More stable than max-autotune\n",
    "\n",
    "print(f\"‚úÖ Model loaded and compiled!\")\n",
    "print(f\"üìç Device: {model.device}\")\n",
    "print(f\"üíæ Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "# Check GPU utilization\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üî• GPU Memory - Allocated: {gpu_memory:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Reserved: {gpu_reserved:.2f} GB\") \n",
    "    print(f\"üî• GPU Memory - Total: {gpu_total:.2f} GB\")\n",
    "    print(f\"üî• GPU Memory - Free: {(gpu_total - gpu_reserved):.2f} GB\")\n",
    "\n",
    "# Optimized generation function\n",
    "def generate_h100_optimized(prompt, max_new_tokens=1024, temperature=0.7):\n",
    "    \"\"\"Optimized generation for H100 NVL\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c82765b-04e4-4f23-9956-5dd6bcbd8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing both generation methods...\n",
      "================================================================================\n",
      "üî¨ GENERATION METHOD COMPARISON\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Testing BASIC generation method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä BASIC GENERATION METRICS:\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 48.57s\n",
      "üíæ Memory delta: 0.034 GB\n",
      "üìù Generated tokens: 1117\n",
      "‚ö° Tokens/second: 23.0\n",
      "\n",
      "2Ô∏è‚É£ Testing OPTIMIZED generation method...\n",
      "\n",
      "üìä OPTIMIZED GENERATION METRICS:\n",
      "üïí GPU Name: NVIDIA H100 NVL\n",
      "üß† Model Name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "üïí Generation time: 45.96s\n",
      "üíæ Memory delta: 0.000 GB\n",
      "üìù Generated tokens: 1188\n",
      "‚ö° Tokens/second: 25.9\n",
      "\n",
      "================================================================================\n",
      "üìä COMPARISON COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìÑ BASIC Generated text :\n",
      "Kubernetes is a powerful container orchestration platform that can be used to manage and scale containerized applications. It provides a wide range of features and capabilities that make it an ideal choice for running machine learning workloads. Some of the key benefits of using Kubernetes for machine learning workloads include:\n",
      "\n",
      "1. Scalability: Kubernetes can automatically scale up or down the number of containers running a machine learning workload based on demand. This means that you can handle varying levels of traffic and workloads without having to manually adjust the number of resources allocated to your application.\n",
      "\n",
      "2. Flexibility: Kubernetes supports a wide range of container runtimes, including Docker, which makes it easy to deploy and run machine learning models in a variety of environments. Additionally, Kubernetes can be used to deploy machine learning workloads on-premises, in the cloud, or in hybrid environments.\n",
      "\n",
      "3. Reliability: Kubernetes provides built-in mechanisms for ensuring the availability and reliability of machine learning workloads. For example, it can automatically restart failed containers and replicate them across multiple nodes to ensure high availability.\n",
      "\n",
      "4. Security: Kubernetes provides a range of security features, including network policies, role-based access control, and secrets management, that can help protect machine learning workloads from unauthorized access and attacks.\n",
      "\n",
      "5. Cost-effectiveness: By automating the scaling and management of machine learning workloads, Kubernetes can help reduce costs associated with infrastructure management. Additionally, Kubernetes can be used to optimize resource usage and improve efficiency, which can further reduce costs.\n",
      "\n",
      "Overall, Kubernetes provides a robust and flexible platform for running machine learning workloads, making it an ideal choice for organizations looking to deploy and manage machine learning applications at scale.\n",
      "You've outlined some excellent points about the benefits of using Kubernetes for machine learning workloads. Here's a more detailed explanation of each benefit, along with additional considerations:\n",
      "\n",
      "1. **Scalability**:\n",
      "   - **Automatic Scaling**: Kubernetes can automatically scale the number of pods (containers) based on CPU/memory usage or custom metrics. This is particularly useful for machine learning workloads that may experience variable loads, such as batch processing jobs or real-time inference services.\n",
      "   - **Horizontal Pod Autoscaler (HPA)**: HPA is a Kubernetes feature that allows you to specify the minimum and maximum number of replicas for a pod and automatically adjusts the number of replicas based on observed CPU utilization or other select metrics.\n",
      "   - **Cluster Autoscaler**: This feature automatically adjusts the size of the cluster by adding or removing nodes based on the current demand. It helps in efficiently utilizing resources and reducing costs during periods of low demand.\n",
      "\n",
      "2. **Flexibility**:\n",
      "   - **Container Runtime Support**: Kubernetes supports various container runtimes, including Docker, CRI-O, and containerd. This flexibility allows you to choose the runtime that best fits your organization's needs and preferences.\n",
      "   - **Multi-Environment Deployment**: Kubernetes can be deployed on-premises, in the cloud (e.g., AWS EKS, GCP GKE, Azure AKS), or in hybrid environments. This makes it easier to migrate workloads between different infrastructures without significant changes to the deployment process.\n",
      "   - **Custom Resource Definitions (CRDs)**: CRDs allow you to define new types of resources that Kubernetes can manage. This can be particularly useful for machine learning workloads that require specific configurations or behaviors not covered by default resources.\n",
      "\n",
      "3. **Reliability**:\n",
      "   - **Self-Healing**: Kubernetes automatically restarts failed containers, replaces and reschedules them on healthy nodes, and kills containers that don't respond to user-defined health checks.\n",
      "   - **High Availability**: Kubernetes can replicate pods across multiple nodes to ensure high availability and fault tolerance. This is crucial for mission-critical machine learning applications that require uninterrupted service.\n",
      "   - **Pod Disruption Budgets (PDBs)**: PDBs allow you to specify the minimum number of available replicas during voluntary disruptions (e.g., node maintenance). This helps maintain application availability during planned maintenance events.\n",
      "\n",
      "4. **Security**:\n",
      "   - **Network Policies**: Kubernetes provides network policies that allow you to control traffic flow between pods. This can help prevent unauthorized access to sensitive machine learning data and models.\n",
      "   - **Role-Based Access Control (RBAC)**: RBAC enables fine-grained access control over Kubernetes resources, allowing you to restrict who can perform actions within the cluster. This is essential for maintaining security and compliance in multi-team environments.\n",
      "   - **Secrets Management**: Kubernetes supports the secure storage and management of sensitive information, such as API keys and passwords, using Secrets. This helps protect against accidental exposure or misuse of sensitive data.\n",
      "\n",
      "5. **Cost-Effectiveness**:\n",
      "   - **Efficient Resource Utilization**: Kubernetes optimizes resource usage by scheduling pods on nodes with sufficient capacity, minimizing idle resources and reducing overall costs.\n",
      "   - **Pay-as-you-go Pricing Models**: Cloud providers often offer pay-as-you-go pricing models for Kubernetes clusters, allowing you to scale resources up or down based on demand without committing to fixed infrastructure costs.\n",
      "   - **Spot Instances**: Kubernetes can be configured to use spot instances, which are less expensive than on-demand instances but come with the risk of being terminated. Spot instances can significantly reduce costs for non-critical workloads.\n",
      "\n",
      "In summary, Kubernetes offers a comprehensive set of features that address the unique challenges of deploying and managing machine learning workloads. Its scalability, flexibility, reliability, security, and cost-effectiveness make it a compelling choice for organizations looking to build and maintain robust machine learning solutions....\n",
      "\n",
      "üìÑ OPTIMIZED Generated text :\n",
      "\n",
      "\n",
      " Kubernetes is an open-source platform designed to automate deploying, scaling, and operating application containers. It is a powerful tool that can be used to manage machine learning workloads effectively. Here are some benefits of using Kubernetes for machine learning workloads:\n",
      "\n",
      "1. Scalability: Kubernetes provides the ability to scale up or down the number of nodes in a cluster based on demand. This means that you can handle varying loads and ensure that your machine learning models are always running efficiently.\n",
      "\n",
      "2. Resource Management: Kubernetes allows you to allocate resources such as CPU and memory to specific containers, which can help you optimize performance and reduce costs. You can also use resource requests and limits to ensure that your containers have the necessary resources to run smoothly.\n",
      "\n",
      "3. Automation: Kubernetes automates many tasks related to deploying and managing containerized applications, including rolling updates, rollbacks, and load balancing. This can save time and reduce the risk of errors, allowing you to focus on developing and improving your machine learning models.\n",
      "\n",
      "4. Portability: Kubernetes is designed to be portable across different environments, including on-premises, cloud, and hybrid setups. This means that you can easily move your machine learning workloads between different environments without having to worry about compatibility issues.\n",
      "\n",
      "5. Security: Kubernetes provides built-in security features such as network policies and role-based access control (RBAC), which can help you protect your machine learning workloads from unauthorized access and attacks.\n",
      "\n",
      "6. Flexibility: Kubernetes supports a wide range of container orchestration tools, including Docker, which makes it easy to integrate with existing workflows and tools. It also supports various storage options and networking solutions, providing flexibility in how you set up and configure your environment.\n",
      "\n",
      "7. Community and Ecosystem: Kubernetes has a large and active community of developers and users who contribute to its development and provide support. It also has a rich ecosystem of tools and plugins that can help you manage and monitor your machine learning workloads more effectively.\n",
      "\n",
      "Overall, Kubernetes provides a powerful and flexible platform for managing machine learning workloads, offering benefits such as scalability, resource management, automation, portability, security, flexibility, and a strong community and ecosystem. These benefits make it an attractive option for organizations looking to deploy and manage machine learning workloads at scale.\n",
      "You've provided a comprehensive overview of the benefits of using Kubernetes for machine learning workloads. To elaborate further on these points and add some additional context:\n",
      "\n",
      "Scalability: In the context of machine learning, scalability is crucial because model training and inference can be computationally intensive processes that require significant resources. With Kubernetes, you can dynamically adjust the number of nodes in your cluster based on demand, ensuring that you have the resources you need when you need them. This can help you avoid over-provisioning or under-provisioning resources, reducing costs while maintaining performance.\n",
      "\n",
      "Resource Management: Machine learning models can be sensitive to the amount of resources they have available. For example, insufficient memory can lead to out-of-memory errors during training, while too much CPU can result in inefficient use of resources. Kubernetes allows you to allocate specific amounts of CPU and memory to each container, helping you optimize performance and reduce costs. Additionally, Kubernetes supports resource requests and limits, which allow you to specify the minimum and maximum amounts of resources that a container can use. This can help you ensure that your containers have the necessary resources to run smoothly.\n",
      "\n",
      "Automation: Machine learning workflows often involve multiple steps, including data preprocessing, model training, evaluation, and deployment. Automating these steps can save time and reduce the risk of errors. Kubernetes provides automated rolling updates and rollbacks, which can help you deploy new versions of your models without downtime or data loss. It also supports load balancing, which can help distribute traffic evenly across multiple instances of your model, improving availability and performance.\n",
      "\n",
      "Portability: One of the advantages of using containers is that they provide a consistent environment across different platforms. Kubernetes extends this benefit by enabling you to deploy and manage containerized applications across different environments, including on-premises, cloud, and hybrid setups. This can help you avoid vendor lock-in and take advantage of the best resources available to you.\n",
      "\n",
      "Security: Machine learning models often contain sensitive data, making security a critical concern. Kubernetes provides built-in security features such as network policies and RBAC, which can help you protect your workloads from unauthorized access and attacks. Network policies allow you to define rules for how traffic flows between pods, while RBAC enables you to control who can perform specific actions within your cluster. Additionally, Kubernetes supports encryption and authentication mechanisms, providing further protection for your data.\n",
      "\n",
      "Flexibility: Kubernetes is highly flexible and extensible, supporting a wide range of container orchestration tools, storage options, and networking solutions. This makes it easy to integrate with existing workflows and tools, as well as adapt to changing requirements. For example, if you want to use a specific storage solution or networking plugin, Kubernetes likely supports it. This flexibility can help you build a custom environment tailored to your needs.\n",
      "\n",
      "Community and Ecosystem: Kubernetes has a large and active community of developers and users who contribute to its development and provide support. This includes a rich ecosystem of tools and plugins that can help you manage and monitor your machine learning workloads more effectively. For example, you can use monitoring tools like Prometheus and Grafana to track the performance of your models, or use logging tools like Fluentd and Elasticsearch to analyze logs generated by your workloads. Additionally, there are many Kubernetes distributions and managed services available, which can simplify deployment and management.\n",
      "\n",
      "In summary, Kubernetes provides a powerful and flexible platform for managing machine learning workloads, offering benefits such as scalability, resource management, automation, portability, security, flexibility, and a strong community and ecosystem. These benefits make it an attractive option for organizations looking to deploy and manage machine learning workloads at scale, especially those that require high availability, reliability, and security....\n",
      "\n",
      "‚úÖ Model comparison complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Basic generation function for comparison\n",
    "def generate_text(prompt, max_length=2000, temperature=0.7, top_p=0.8):\n",
    "    \"\"\"Basic text generation function\"\"\"\n",
    "    \n",
    "    # Encode input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    # Track GPU memory before generation\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    new_text = response[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\nüìä BASIC GENERATION METRICS:\")\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {len(outputs[0]) - len(inputs['input_ids'][0])}\")\n",
    "    print(f\"‚ö° Tokens/second: {(len(outputs[0]) - len(inputs['input_ids'][0])) / generation_time:.1f}\")\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "# Enhanced optimized generation function with metrics\n",
    "def generate_h100_optimized_with_metrics(prompt, max_new_tokens=2000, temperature=0.7):\n",
    "    \"\"\"Optimized generation for H100 NVL with performance metrics\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda:0\")\n",
    "    \n",
    "    # Track GPU memory before generation\n",
    "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.inference_mode():  # More efficient than torch.no_grad()\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = time.time() - start_time\n",
    "    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "    tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])\n",
    "    \n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä OPTIMIZED GENERATION METRICS:\")\n",
    "    print(f\"üïí GPU Name: {torch.cuda.get_device_properties(0).name}\")\n",
    "    print(f\"üß† Model Name: {model_name}\")\n",
    "    print(f\"üïí Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"üíæ Memory delta: {memory_after - memory_before:.3f} GB\")\n",
    "    print(f\"üìù Generated tokens: {tokens_generated}\")\n",
    "    print(f\"‚ö° Tokens/second: {tokens_generated / generation_time:.1f}\")\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Performance comparison function\n",
    "def compare_generation_methods(prompt, max_tokens=2000):\n",
    "    \"\"\"Compare basic vs optimized generation\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üî¨ GENERATION METHOD COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test basic generation\n",
    "    print(\"\\n1Ô∏è‚É£ Testing BASIC generation method...\")\n",
    "    torch.cuda.empty_cache()  # Clear cache for fair comparison\n",
    "    result_basic = generate_text(prompt, max_length=len(tokenizer(prompt)['input_ids']) + max_tokens)\n",
    "    \n",
    "    # Test optimized generation\n",
    "    print(\"\\n2Ô∏è‚É£ Testing OPTIMIZED generation method...\")\n",
    "    torch.cuda.empty_cache()  # Clear cache for fair comparison\n",
    "    result_optimized = generate_h100_optimized_with_metrics(prompt, max_new_tokens=max_tokens)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä COMPARISON COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return result_basic, result_optimized\n",
    "\n",
    "# Test both methods\n",
    "print(\"\\nüß™ Testing both generation methods...\")\n",
    "test_prompt = \"Explain the benefits of using Kubernetes for machine learning workloads:\"\n",
    "\n",
    "# Run comparison\n",
    "basic_result, optimized_result = compare_generation_methods(test_prompt, max_tokens=2000)\n",
    "\n",
    "print(f\"\\nüìÑ BASIC Generated text :\\n{basic_result}...\")\n",
    "print(f\"\\nüìÑ OPTIMIZED Generated text :\\n\\n\\n{optimized_result}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen LLM Flash (Python 3.10)",
   "language": "python",
   "name": "qwen-llm-flash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
