{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674a3b76",
   "metadata": {},
   "source": [
    "# Test Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011f6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "Transformers version: 4.53.2\n",
      "Datasets version: 4.0.0\n",
      "CUDA available: False\n",
      "MPS (Metal Performance Shaders) available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: [{'label': 'POSITIVE', 'score': 0.99968421459198}]\n"
     ]
    }
   ],
   "source": [
    "# test_installation.py\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS (Metal Performance Shaders) available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Test a simple pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love learning about AI!\")\n",
    "print(f\"Test result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047856ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    pipeline, \n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfcfcb",
   "metadata": {},
   "source": [
    "# Set device for Mac M1 optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd8bd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd83ad",
   "metadata": {},
   "source": [
    "# EXERCISE 1: PIPELINES (High-level API)\n",
    "## 1. Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d26d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=0 if device.type == \"mps\" else -1)\n",
    "    \n",
    "texts = [\n",
    "        \"I love this movie!\",\n",
    "        \"This is terrible.\",\n",
    "        \"It's okay, not great but not bad either.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb79f608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'I love this movie!' -> {'label': 'POSITIVE', 'score': 0.9998775720596313}\n",
      "Text: 'This is terrible.' -> {'label': 'NEGATIVE', 'score': 0.9996345043182373}\n",
      "Text: 'It's okay, not great but not bad either.' -> {'label': 'POSITIVE', 'score': 0.9985427856445312}\n"
     ]
    }
   ],
   "source": [
    "results = sentiment_pipeline(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: '{text}' -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80150089",
   "metadata": {},
   "source": [
    "## 2. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42763fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6369433981a24d47ade3d5156d764f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73916a72a00b4265846c876e1d350570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b53c0ea3f914dd8932e5a87b871b6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd275c5f0864601a90d5ef3978676c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ce69c01f554493a2423a94a2970214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ad685f70d24456980c97798813447d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f4cc46ee5247ce82b41f3a21973682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: The future of artificial intelligence is uncertain. At the moment, it's focused on working on deep learning algorithms -- that is, to learn what the future of AI will look like. But a lot of work still needs to be done, and this paper proposes to do just that. This is a really interesting paper. It's not about the AI problem. But it's also not about the future, and it's not even about the problem of artificial intelligence. It's about the problems with the future of intelligence.\n",
      "\n",
      "One of the most interesting aspects of this paper is that it focuses on the problems of learning how to code, how to write, how to code and how to code. It's not about the data, it's about the data you're creating. It's about what you're going to do in the future. The problem with the data is that it's not necessarily about what you're going to do in the future, but about what's going to be done in the future. The way that people write code in the future is by writing more code. This is what I call the \"data structure problem.\"\n",
      "\n",
      "The problem with the data structure problem is that it's more about the data. And this paper is about the data structure problem. You can write a program for\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=0 if device.type == \"mps\" else -1)\n",
    "    \n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "print(f\"Generated: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9113238",
   "metadata": {},
   "source": [
    "## 3. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f74db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bdcc4ecd5347caa4ddc6a4e0a3da10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c0d01259a6459dac2bd8119cb21fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d3900cdb3c48b78bfbc4cecd02f359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41debcf882684a4bb598d04257ee6bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5807da3ab0c45bb8706adddc098c9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Hugging Face known for?\n",
      "Answer: Transformers library (confidence: 0.7477)\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", device=0 if device.type == \"mps\" else -1)\n",
    "    \n",
    "context = \"Hugging Face is a company that develops tools for machine learning. They are known for their Transformers library.\"\n",
    "question = \"What is Hugging Face known for?\"\n",
    "    \n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer['answer']} (confidence: {answer['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c294c8",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3551ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4b79817e4c4a81a79c986371c67dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fce9e3db1e4fe89443bbe54173d168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cac5b903f214caea462a5002492b544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092ab18c1d144f919abf0ab835252c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: 'Apple Inc' - Type: ORG (confidence: 0.9996)\n",
      "Entity: 'Steve Jobs' - Type: PER (confidence: 0.9892)\n",
      "Entity: 'Cupertino' - Type: LOC (confidence: 0.9711)\n",
      "Entity: 'California' - Type: LOC (confidence: 0.9989)\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=0 if device.type == \"mps\" else -1)\n",
    "    \n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "entities = ner_pipeline(text)\n",
    "    \n",
    "for entity in entities:\n",
    "    print(f\"Entity: '{entity['word']}' - Type: {entity['entity_group']} (confidence: {entity['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494b964",
   "metadata": {},
   "source": [
    "## 5. NSFW Image Classification - Fine-Tuned Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed993c91",
   "metadata": {},
   "source": [
    "### Method 1: Pipeline (Easiest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb455fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image classification result: [{'label': 'normal', 'score': 0.9998319149017334}, {'label': 'nsfw', 'score': 0.0001680291461525485}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\")\n",
    "result = classifier(\"/Users/tharindu/Downloads/1747220469461.jpeg\")\n",
    "print(f\"Image classification result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cbf4e",
   "metadata": {},
   "source": [
    "### Method 2: Manual Control (More Flexible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2276448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. RAM used: 8175.69 MB\n",
      "Predicted class: normal\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "# Load the model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(\"Falconsai/nsfw_image_detection\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"Falconsai/nsfw_image_detection\")\n",
    "\n",
    "print(f\"Model loaded. RAM used: {psutil.virtual_memory().used / (1024**2):.2f} MB\")\n",
    "\n",
    "# Load your image (replace 'your_image.jpg' with your actual image path)\n",
    "image = Image.open(\"/Users/tharindu/Downloads/1747220469461.jpeg\").convert(\"RGB\")\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predicted class\n",
    "logits = outputs.logits\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "labels = model.config.id2label\n",
    "print(f\"Predicted class: {labels[predicted_class_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1212f88",
   "metadata": {},
   "source": [
    "# ONNX (Open Neural Network Exchange)\n",
    "\n",
    "**ONNX (Open Neural Network Exchange)** is a universal format for AI models, enabling interoperability across different frameworks.\n",
    "\n",
    "## Why ONNX?\n",
    "\n",
    "- **Framework Agnostic:**  \n",
    "    PyTorch models speak \"PyTorch language\", TensorFlow models speak \"TensorFlow language\"—ONNX acts as a \"Google Translate\" for AI models.\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- ✅ **Universal format:** Works across frameworks and platforms\n",
    "- ✅ **Smaller file sizes:** Often 2–4x smaller than native formats\n",
    "- ✅ **Faster inference:** Especially on CPU (2–5x speedup)\n",
    "- ✅ **No framework dependency:** Run models without needing PyTorch or TensorFlow\n",
    "- ✅ **Production-ready:** Easier deployment in production systems\n",
    "- ✅ **Optimized:** Built-in graph optimizations\n",
    "- ✅ **Hardware support:** Better compatibility with specialized hardware\n",
    "- ✅ **Lower memory usage:** Uses less RAM during inference\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Feature        | Description                                      |\n",
    "|----------------|--------------------------------------------------|\n",
    "| **Speed**      | 2–5x faster inference, especially on CPU         |\n",
    "| **Size**       | Models are typically 50–75% smaller              |\n",
    "| **Portability**| Run on any device/platform without PyTorch       |\n",
    "| **Memory**     | Uses less RAM during inference                   |\n",
    "| **Deployment** | Easier to deploy in production systems           |\n",
    "| **Optimization**| Built-in graph optimizations                    |\n",
    "| **Hardware**   | Better support for specialized hardware          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66191f9b",
   "metadata": {},
   "source": [
    "## Model Formats: SafeTensors vs ONNX\n",
    "\n",
    "### **SafeTensors** (Current Format)\n",
    "- **Type:** PyTorch's secure model format\n",
    "- **Contents:** Model weights + architecture info\n",
    "- **Requirements:** Needs PyTorch to run\n",
    "- **Size:** ~330MB for NSFW model\n",
    "- **Best for:** Development & research\n",
    "\n",
    "---\n",
    "\n",
    "### **ONNX** (Open Neural Network Exchange)\n",
    "- **Type:** Universal format for any framework\n",
    "- **Features:** Optimized computation graph\n",
    "- **Requirements:** Runs with just ONNX Runtime\n",
    "- **Size:** ~150–200MB for same model\n",
    "- **Best for:** Production deployment\n",
    "\n",
    "## Converting to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2786bbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"Falconsai/nsfw_image_detection\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a78337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # batch_size=1, 3 channels, 224x224 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26ba8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharindu/git/huggingface-learning/.venv/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/Users/tharindu/git/huggingface-learning/.venv/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:161: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"falconsai_nsfw.onnx\",          # output filename\n",
    "    input_names=[\"pixel_values\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"pixel_values\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
    "    opset_version=14\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
